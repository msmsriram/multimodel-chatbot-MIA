{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01dcf9e-c8f4-4c34-a013-8fd08d2d3806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain unstructured[all-docs] pydantic lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b56bde-1ba0-4525-a11d-cab02c5659e4",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "### Partition PDF tables, text, and images\n",
    "  \n",
    "* `LLaVA` Paper: https://arxiv.org/pdf/2304.08485.pdf\n",
    "* Use [Unstructured](https://unstructured-io.github.io/unstructured/) to partition elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3826584-1ff5-4d86-911a-a9242aaad5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# Path to save images\n",
    "path = \"C:\\\\Users\\\\Barani\\\\Desktop\\\\chatbot\\\\\"\n",
    "\n",
    "# Get elements\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=path + \"llava.pdf\",\n",
    "    # Using pdf format to find embedded image blocks\n",
    "    extract_images_in_pdf=True,\n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    # Post processing to aggregate text once we have the title\n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    # Hard max on chunks\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cdba921-5419-4471-b234-d93af3859b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"<class 'unstructured.documents.elements.CompositeElement'>\": 31,\n",
       " \"<class 'unstructured.documents.elements.Table'>\": 4}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to store counts of each type\n",
    "category_counts = {}\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    category = str(type(element))\n",
    "    if category in category_counts:\n",
    "        category_counts[category] += 1\n",
    "    else:\n",
    "        category_counts[category] = 1\n",
    "\n",
    "# Unique_categories will have unique elements\n",
    "# TableChunk if Table > max chars set above\n",
    "unique_categories = set(category_counts.keys())\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f660305-e165-4b6c-ada3-a67a422defb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "\n",
    "# Categorize by type\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "# Tables\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "print(len(table_elements))\n",
    "\n",
    "# Text\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "print(len(text_elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa7f52f-bf5c-4ba4-af72-b2ccba59a4cf",
   "metadata": {},
   "source": [
    "## Multi-vector retriever\n",
    "\n",
    "Use [multi-vector-retriever](/docs/modules/data_connection/retrievers/multi_vector#summary).\n",
    "\n",
    "Summaries are used to retrieve raw tables and / or raw chunks of text.\n",
    "\n",
    "### Text and Table summaries\n",
    "\n",
    "Here, we use Ollama to run LLaMA2 locally. \n",
    "\n",
    "See details on installation [here](/docs/guides/development/local_llms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "523e6ed2-2132-4748-bdb7-db765f20648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22c22e3f-42fb-4a4a-a87a-89f10ba8ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "model = ChatOllama(model=\"mistral\",temperature=0)\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e1ba7ba-d209-424a-8f05-6a95d6d32bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to text\n",
    "texts = [i.text for i in text_elements if i.text != \"\"]\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a419123a-6038-4264-9ee0-bfb2a2df7153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to tables\n",
    "tables = [i.text for i in table_elements]\n",
    "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8caf253d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This table and text describe a study conducted by Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee, where they introduce LLaVA (Large Language and Vision Assistant), an end-to-end trained large multimodal model that connects a vision encoder and a language model for general-purpose visual and language understanding. They used GPT-4 to generate multimodal instruction-following data, which was then used for instruction tuning. The experiments showed that LLaVA demonstrated impressive multimodal chat abilities and yielded a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieved a new state-of-the-art accuracy of 92.53%. The researchers constructed two evaluation benchmarks for future research on visual instruction following and made their data, model, and code publicly available.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfde37cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This table summarizes the performance of a conversational model in handling different levels of conversation complexity and detail. The first column represents the conversation type, with \"Detail description\" indicating a conversation with full data and detailed descriptions, \"Complex Conv\" representing complex conversations, and \"No Instruction Tuning\" denoting conversations without specific instructions. The following columns display the model\\'s performance in terms of accuracy percentage for each conversation type.\\n\\nThe results show that the conversational model performs best when handling detailed descriptions with no instruction tuning (96.5%), but its accuracy decreases significantly as complexity increases, particularly for complex conversations with full data (18.5%). The overall trend indicates a decline in performance as both detail and complexity levels rise.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_summaries[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d52641eb-762e-4460-80c7-3ac3ddd93621",
   "metadata": {},
   "source": [
    "### Images\n",
    "\n",
    "We will implement `Option 2` discussed above: \n",
    "\n",
    "* Use a multimodal LLM ([LLaVA](https://llava.hliu.cc/)) to produce text summaries from images\n",
    "* Embed and retrieve text \n",
    "* Pass text chunks to an LLM for answer synthesis \n",
    "\n",
    "#### Image summaries \n",
    "\n",
    "We will use [LLaVA](https://github.com/haotian-liu/LLaVA/), an open source multimodal model.\n",
    " \n",
    "We will use [llama.cpp](https://github.com/ggerganov/llama.cpp/pull/3436) to run LLaVA locally (e.g., on a Mac laptop):\n",
    "\n",
    "* Clone [llama.cpp](https://github.com/ggerganov/llama.cpp)\n",
    "* Download the LLaVA model: `mmproj-model-f16.gguf` and one of `ggml-model-[f16|q5_k|q4_k].gguf` from [LLaVA 7b repo](https://huggingface.co/mys/ggml_llava-v1.5-7b/tree/main)\n",
    "* Build\n",
    "```\n",
    "mkdir build && cd build && cmake ..\n",
    "cmake --build .\n",
    "```\n",
    "* Run inference across images:\n",
    "```\n",
    "/Users/rlm/Desktop/Code/llama.cpp/bin/llava -m ../models/llava-7b/ggml-model-q5_k.gguf --mmproj ../models/llava-7b/mmproj-model-f16.gguf --temp 0.1 -p \"Describe the image in detail. Be specific about graphs, such as bar plots.\" --image \"$img\" > \"$output_file\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "646a6874-008e-46aa-809d-1d59df36858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# # Define the directory containing the images\n",
    "# IMG_DIR=~/Desktop/Papers/LLaVA/\n",
    "\n",
    "# # Loop through each image in the directory\n",
    "# for img in \"${IMG_DIR}\"*.jpg; do\n",
    "#     # Extract the base name of the image without extension\n",
    "#     base_name=$(basename \"$img\" .jpg)\n",
    "\n",
    "#     # Define the output file name based on the image name\n",
    "#     output_file=\"${IMG_DIR}${base_name}.txt\"\n",
    "\n",
    "#     # Execute the command and save the output to the defined output file\n",
    "#     /Users/rlm/Desktop/Code/llama.cpp/bin/llava -m ../models/llava-7b/ggml-model-q5_k.gguf --mmproj ../models/llava-7b/mmproj-model-f16.gguf --temp 0.1 -p \"Describe the image in detail. Be specific about graphs, such as bar plots.\" --image \"$img\" > \"$output_file\"\n",
    "\n",
    "# done\n",
    "# # %%bash\n",
    "\n",
    "# # # Define the directory containing the images\n",
    "# # IMG_DIR=~/Desktop/Papers/LLaVA/\n",
    "\n",
    "# # # Loop through each image in the directory\n",
    "# # for img in \"${IMG_DIR}\"*.jpg; do\n",
    "# #     # Extract the base name of the image without extension\n",
    "# #     base_name=$(basename \"$img\" .jpg)\n",
    "\n",
    "# #     # Define the output file name based on the image name\n",
    "# #     output_file=\"${IMG_DIR}${base_name}.txt\"\n",
    "\n",
    "# #     # Execute the command and save the output to the defined output file\n",
    "# #     # /Users/rlm/Desktop/Code/llama.cpp/bin/llava -m ../models/llava-7b/ggml-model-q5_k.gguf --mmproj ../models/llava-7b/mmproj-model-f16.gguf --temp 0.1 -p \"Describe the image in detail. Be specific about graphs, such as bar plots.\" --image \"$img\" > \"$output_file\"\n",
    "# #     C:\\\\Users\\\\Barani\\\\Desktop\\\\chatbot\\\\llama.cpp\\\\examples\\\\llava -m ..C:\\\\Users\\\\Barani\\\\Desktop\\\\chatbot\\\\ggml-model-q4_k.gguf --mmproj ..C:\\\\Users\\\\Barani\\\\Desktop\\\\chatbot\\\\mmproj-model-f16.gguf --temp 0.1 -p \"Describe the image in detail. Be specific about graphs, such as bar plots.\" --image \"%img%\" > \"%output_file%\"\n",
    "\n",
    "\n",
    "# # done\n",
    "\n",
    "# # # Define the directory containing the images\n",
    "# # IMG_DIR = \"C:\\Users\\YourUsername\\Desktop\\Papers\\LLaVA\\\"\n",
    "\n",
    "# # # Get all the .jpg files in the directory\n",
    "# # images = Get-ChildItem -Path IMG_DIR -Filter *.jpg\n",
    "\n",
    "# # # Loop through each image in the directory\n",
    "# # foreach ($img in $images) {\n",
    "# #     # Extract the base name of the image without extension\n",
    "# #     base_name = [IO.Path]::GetFileNameWithoutExtension($img.Name)\n",
    "\n",
    "# #     # Define the output file name based on the image name\n",
    "# #     output_file = \"${IMG_DIR}${base_name}.txt\"\n",
    "\n",
    "# #     # Execute the command and save the output to the defined output file\n",
    "# #     & \"C:\\Users\\YourUsername\\Desktop\\Code\\llama.cpp\\bin\\llava.exe\" -m \"..\\models\\llava-7b\\ggml-model-q5_k.gguf\" --mmproj \"..\\models\\llava-7b\\mmproj-model-f16.gguf\" --temp 0.1 -p \"Describe the image in detail. Be specific about graphs, such as bar plots.\" --image \"$img.FullName\" > \"$output_file\"\n",
    "# # }\n",
    "# # import os\n",
    "# # import subprocess\n",
    "\n",
    "# # # Define the directory containing the images\n",
    "# # IMG_DIR = \"C:\\\\Users\\\\Barani\\\\Desktop\\\\chatbot\\\\figures\"\n",
    "\n",
    "# # # Get all the .jpg files in the directory\n",
    "# # images = [f for f in os.listdir(IMG_DIR) if f.endswith('.jpg')]\n",
    "\n",
    "# # # Loop through each image in the directory\n",
    "# # for img in images:\n",
    "# #     # Extract the base name of the image without extension\n",
    "# #     base_name = os.path.splitext(img)[0]\n",
    "\n",
    "# #     # Define the output file name based on the image name\n",
    "# #     output_file = os.path.join(IMG_DIR, f\"{base_name}.txt\")\n",
    "\n",
    "# #     # Define the command\n",
    "# #     command = f\"C:/Users/YourUsername/Desktop/Code/llama.cpp/bin/llava.exe -m ../models/llava-7b/ggml-model-q5_k.gguf --mmproj ../models/llava-7b/mmproj-model-f16.gguf --temp 0.1 -p \\\"Describe the image in detail. Be specific about graphs, such as bar plots.\\\" --image {os.path.join(IMG_DIR, img)} > {output_file}\"\n",
    "\n",
    "# #     # Execute the command\n",
    "# #     subprocess.run(command, shell=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a7aecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 16/16 [07:32<00:00, 28.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' The image shows a humorous meme with a text overlay that reads, \"sometimes I just look at pictures of the earth from space and marvel at how beautiful it all is.\" Below this text is a photo of a tray containing pieces of chicken or fish sticks, which are arranged to form the shape of a map. The arrangement resembles continents on Earth. The meme uses a visual pun by juxtaposing a thoughtful statement with an image of food that has been creatively shaped to mimic a world map from space. The text suggests a sense of wonder and appreciation for the natural world, while the image plays on this sentiment using food as a medium for art. ', ' The image shows a screenshot of a text message with a conversation between two individuals. One person has sent a message that says \"My new website is live!\" along with a link to the site. In response, another person replies \"That looks funny\" and includes a link to a meme, specifically a joke about punctuation marks.\\n\\nThe screenshot also includes a conversation notification at the top, indicating that there are two messages in the conversation: one from Leva saying \"My new website is live!\" and the other from My Joke Website. Below these notifications, you can see the message content and links to the websites mentioned.\\n\\nThe image appears to be taken from a smartphone displaying this interaction within a messaging app. The meme link points to a website that contains jokes, which is humorously related to the punctuation joke in the conversation. ', ' The image appears to be a screenshot of a chat or messaging application, where the conversation includes text and images. On the left side of the screen is a chat with a user interface design, including what seems to be a text box with a message that says: \"With the same type of ingredients as you have used them in a recipe using food storage containers or bottles.\"\\n\\nThe right side of the screen shows an image of a refrigerator. The bottom part of the screen includes another message saying: \"You can download this for free from my website. Just visit my website and fill out the form for the free download link.\"\\n\\nThe overall style of the image suggests that it is meant to represent a user-friendly experience with a chatbot or an AI-based system capable of assisting users in organizing their ingredients using food storage containers or bottles. The presence of the refrigerator image may indicate a discussion about food organization and storage tips. ', ' The image is a screenshot of a conversation within a chat application. It features a series of text bubbles that appear to be messages from one user to another. The first message says, \"WHEN THE THINGS YOU SAY DON\\'T MAKE SENSE TO ME.\" This suggests that the sender might be feeling misunderstood or frustrated about the communication.\\n\\nThe second message reads, \"I\\'LL BE HONEST WITH YOU ABOUT THE STUFF I DIDN\\'T LIKE.\" This indicates a willingness to provide constructive feedback and highlights the importance of open communication.\\n\\nThe third message states, \"I\\'D RATHER YOU TELL ME THAN MAKE YOU FEEL LIKE I NEED TO DO IT FOR YOU.\" This shows that the sender values honesty and respects the other person\\'s ability to express their thoughts.\\n\\nIn addition to the text, there is a small image of a serene landscape with a lake and mountains in the background, located at the top left corner of the chat. The user interface elements such as icons for attachments or reactions are visible along the bottom, indicating that this platform supports various forms of communication beyond just text messages.\\n\\nThe watermark \"LOVEWATER\" is present on the image, suggesting that it may have been taken from a device with a custom wallpaper set by the user. ', ' The image shows a computer screen with a blurred background and a text box prominently displayed in the foreground. The text is not clear, but it appears to be related to a conversation or presentation about a movie or animation, as suggested by phrases like \"the magic of storytelling\" and references to characters from \"Star Wars.\" The screen also contains an overlaid graphic of two characters on what looks like a spaceship, which could indicate the theme or context of the text discussion. In the top right corner of the image, there is a blurred image of a person\\'s profile picture. ', ' The image shows a screenshot of a messaging application with two individuals engaged in a conversation. On the left side, there is a message from a person named \"Law\", which includes text and an attached photo. The attached photo is of the famous painting \"The Mona Lisa\" by Leonardo da Vinci, depicting a woman with a mysterious smile.\\n\\nOn the right side of the conversation, there\\'s a reply from another user, who seems to be using a voice assistant or AI, as indicated by the text bubble that says \"You know what, I\\'m feeling pretty inspired now and I want you to show me some really cool art pieces.\" The response from the AI includes a photo of a dog with an artistic twist—the dog appears to be painted in a style reminiscent of famous art movements.\\n\\nThe image also includes a humorous text bubble that says \"I\\'m not sure what you mean by \\'really cool art pieces.\\' Do you want me to find some that have nothing to do with cats?\" This suggests the AI might have initially provided cat-related content, and the user is expressing a preference for dog-themed art. ', ' The image appears to be a screenshot of a social media platform, possibly Instagram or Facebook, given the layout and interface. It shows two separate posts or interactions on the same platform.\\n\\nOn the left, there is a post that includes an image of a man standing in front of a microphone with the caption \"What are your favorite hobbies?\"\\n\\nOn the right side, there are two distinct elements:\\n\\n1. A comment from a user named \"Lava\" responding to the original post. The comment says, \"The name of the man in the photo.\" This suggests that the person who posted the original image might be sharing their experience of a game or event and asking for other people\\'s experiences with similar activities.\\n\\n2. Below the comment from Lava, there is a reply from another user named \"Iron Man,\" which is an inside joke or reference to a character from Marvel Comics. The response says, \"The man in the photo is in the air.\" This could imply that the person being discussed by Lava has some kind of ability to fly or be in mid-air, adding a humorous and fantastical element to the interaction.\\n\\nThe interface elements like the search bar, the profile picture placeholders, and the notification icons are standard for the platform it is set on. The overall tone of the image suggests a casual and friendly conversation between users on social media. ', \" The image shows a wooden rocking chair with a high back. It has a simple, classic design, with vertical spindles along the backrest and arms, and a curved seat bottom. The chair's color is a warm brown, suggesting it may be made from natural wood. In the background, there's a plain light grey gradient which provides a neutral contrast to the chair, making it stand out clearly. There are no texts or additional graphics in the image. \", ' The image shows a screenshot of a document or text file with the heading \"Describe the following image.\" Below this header is a list of instructions for describing an image. There are bullet points that suggest how to describe various aspects of an image, such as providing details about the image\\'s content and context, as well as noting the use of color in the image. The list also suggests considering the presence of objects or people and their relative positions, as well as the overall impression given by the image.\\n\\nThe text is quite small due to the resolution of the image, but it seems to be a structured approach to describing images, with an emphasis on including specific details that might be relevant to understanding the image. The document appears to be designed for educational or instructional purposes, possibly teaching someone how to write descriptive sentences about visual content. ', ' This image is a screenshot of a computer screen displaying a graph and some text. The graph is a bar chart with multiple bars, each representing different categories. There are two y-axes on the left side of the chart, with one labeled \"CM1082\" and the other \"CM1092.\" Each axis has numerical values ranging from 0 to 5, but without further context, it\\'s unclear what these values represent.\\n\\nThe x-axis is labeled \"Frequency,\" which suggests that each bar represents a frequency of something. The bars are different lengths, indicating the number of occurrences for each category.\\n\\nOn the right side of the chart, there is a text box with two lines. The top line reads \"10000\" and below it is the word \"filtered,\" suggesting that this might be a label for one of the bars on the chart. There are additional numbers and phrases in the text box that seem to relate to the data or analysis associated with the graph.\\n\\nThe rest of the image, which occupies most of the space, appears to be a blurred or out-of-focus representation of what might be another graph or set of data, but it\\'s not clear due to the lack of focus and detail. The overall style of the image suggests that it is a technical or statistical visualization, possibly related to scientific research, data analysis, or a similar field. ', ' The image is a screenshot of a computer program displaying an overhead view of a car being tracked. Various pieces of equipment, such as a handheld scanner and other devices, are superimposed on the scene, indicating their locations relative to each other. This appears to be a surveillance or tracking setup, possibly for security purposes or a research project. The car is parked inside what looks like an underground parking garage, with several people present in the space, one of whom is carrying a backpack and another is holding a handbag. There are also various objects scattered around, including bags, bottles, and what seem to be sports equipment. ', ' The image depicts a scene with an overlaid heat map. It appears to be a surveillance or security camera setup in what seems to be a garage, as evidenced by the concrete floor and visible infrastructure. On the left side of the image, there is an object that resembles a white car.\\n\\nThe heat map includes various colored zones that highlight areas of interest. These zones are color-coded with different colors such as red, green, and yellow, which typically represent different levels of activity or temperature in thermal imaging systems.\\n\\nIn the center of the image, there is a person standing near what seems to be a white truck with an open door, suggesting they might be interacting with it. The person is wearing a black jacket and dark pants, and appears to be carrying something in their hand, possibly a small bag or a package.\\n\\nIn the foreground, there are two backpacks placed on the ground. One of them has a distinctive yellow color, while the other has a more neutral shade. Additionally, there is some sports equipment near the person, including what looks like a tennis racket and a pair of white shoes with black accents, possibly indicating that the person might be involved in a sporting activity or preparing for one.\\n\\nThe overall image seems to be a composite of multiple snapshots from different cameras, combined into one frame to provide a comprehensive view of the activities and objects present within the monitored area. ', ' The image appears to be a screenshot or a photo of a presentation slide or a page from a technical manual. The content of the slide is related to language modeling and artificial intelligence, as indicated by the diagrams and text.\\n\\nAt the top, there are three diagrams illustrating a process: on the left, \"Input,\" in the middle, \"Language Model,\" and on the right, \"Output.\" These diagrams seem to represent the flow of data or information through a language model system. The central diagram shows two blocks with arrows pointing between them, suggesting that the process is iterative or involves some sort of feedback loop.\\n\\nBelow these diagrams, there are two sections of text: one on the left and one on the right. The text on the left reads \"Language response\" and includes a mathematical formula labeled as \"H_t\". The text on the right mentions \"projection W,\" which likely refers to a weight matrix or a transformation matrix used in machine learning, and also references \"vision encoder\" and \"language instruction.\"\\n\\nThe slide is part of an educational context or a technical explanation, possibly for an audience interested in AI, data processing, or computational linguistics. ', ' This image depicts a humorous scene in an urban setting. In the foreground, there\\'s a person seated on a cart with clothes, giving the impression of laundry day activities. The cart is being pulled by a yellow taxi cab, which is driving on a street lined with buildings and vehicles. \\n\\nIn the background, there\\'s a bustling scene with a building labeled \"CITI CORP\" and another taxi in motion. The image captures a moment of everyday city life with an element of whimsy added by the person on the cart. There are no visible texts that provide additional context or information about the image. ', \" The image displays a dining table with several dishes of Japanese cuisine. On the left side, there is a bowl of white rice garnished with what appears to be pickled vegetables. Next to it is a small dish containing green and pink seasoning, possibly to add to the rice or other dishes. \\n\\nIn the center, a large bowl holds a serving of ramen noodles. The ramen has a variety of ingredients including slices of meat, what looks like boiled eggs, and green onions. There are also bean sprouts and a red sauce that is likely to be a type of chili oil, which is common in ramen dishes.\\n\\nOn the right side of the table, there's another bowl containing a green vegetable salad with sesame seeds. Beside it is another dish with fried seaweed, a popular snack or appetizer in Japan. \\n\\nThe bowls are decorated with intricate designs, and there are small dishes for additional sauces. The table also has a few condiments, such as soy sauce and possibly a mayonnaise-based sauce, which can be used to enhance the flavors of the dishes. The image showcases a meal that is typical of Japanese cuisine, with a variety of textures and colors suggesting a balance of protein, vegetables, and carbs. \", \" The image depicts an open refrigerator, which is well-stocked with various food items. On the top shelf, there are several containers holding different types of produce and a carton of eggs. Below that, on the second shelf, we can see more food items including berries and possibly some leftovers in plastic containers.\\n\\nThe bottom shelves have an assortment of food, with what appears to be yogurt, milk, and additional fresh produce such as carrots. The fridge door is open, revealing a section of the refrigerator with more items including a carton of juice and some bottles that could contain drinks or condiments.\\n\\nThe image has a blurry effect in the background and on some parts of the food items, suggesting motion or focus issues when the photograph was taken. The fridge itself is a standard model, and there's no visible text on any of the items inside. \"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "img_summaries = []\n",
    "\n",
    "# Define the directory path\n",
    "dir_path = \"C:\\\\Users\\\\Barani\\\\Desktop\\\\chatbot\\\\figures\"\n",
    "\n",
    "# Get a list of all .jpg files in the directory\n",
    "jpg_files = [f for f in os.listdir(dir_path) if f.endswith(\".jpg\")]\n",
    "\n",
    "# Iterate over all .jpg files in the directory with a tqdm progress bar\n",
    "for filename in tqdm(jpg_files, desc=\"Processing images\"):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(dir_path, filename)\n",
    "    \n",
    "    # Call the chat function with the image file\n",
    "    res = ollama.chat(\n",
    "        model=\"llava\",\n",
    "        messages=[\n",
    "            {\n",
    "                'role':'user',\n",
    "                'content':'Describe this image :',\n",
    "                'images':[file_path]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Append the result to the image_summary list\n",
    "    img_summaries.append(res['message']['content'])\n",
    "\n",
    "# Print the image_summary list\n",
    "print(img_summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da8a8c94-3df7-446f-9a69-703295f50f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import os\n",
    "\n",
    "# # Get all .txt files in the directory\n",
    "# file_paths = glob.glob(os.path.expanduser(os.path.join(path, \"*.txt\")))\n",
    "\n",
    "# # Read each file and store its content in a list\n",
    "# img_summaries = []\n",
    "# for file_path in file_paths:\n",
    "#     with open(file_path, \"r\") as file:\n",
    "#         img_summaries.append(file.read())\n",
    "\n",
    "# # Clean up residual logging\n",
    "# cleaned_img_summary = [\n",
    "#     s.split(\"clip_model_load: total allocated memory: 201.27 MB\\n\\n\", 1)[1].strip()\n",
    "#     for s in img_summaries\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b030d4-2ac5-41b6-9245-fc3ba5771d87",
   "metadata": {},
   "source": [
    "### Add to vectorstore\n",
    "\n",
    "Use [Multi Vector Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#summary) with summaries.\n",
    "\n",
    "We use GPT4All embeddings to run locally, which are a [CPU optimized version of BERT](https://docs.gpt4all.io/gpt4all_python_embedding.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b545d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community import embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64a5df0c-8193-407e-a83f-8fc17caff3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"summaries\", embedding_function=GPT4AllEmbeddings(),persist_directory=\"./chroma\"\n",
    ")\n",
    "vectorstore.persist()\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()  # <- Can we extend this to images\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339bb8be-0d7a-45a0-8815-d62bb3bbf0fc",
   "metadata": {},
   "source": [
    "For `option 2` (above): \n",
    "\n",
    "* Store the image summary in the `docstore`, which we return to the LLM for answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d643cc61-827d-4f3c-8242-7a7c8291ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(text_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "    for i, s in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "\n",
    "# Add images\n",
    "img_ids = [str(uuid.uuid4()) for _ in img_summaries]\n",
    "summary_img = [\n",
    "    Document(page_content=s, metadata={id_key: img_ids[i]})\n",
    "    for i, s in enumerate(img_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_img)\n",
    "retriever.docstore.mset(\n",
    "    list(zip(img_ids, img_summaries))\n",
    ")  # Store the image summary as the raw document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45fb81-46b1-426e-aa2c-01aed4eac700",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb23d5-ae66-444d-8f5f-b24107fb9c57",
   "metadata": {},
   "source": [
    "Image:"
   ]
  },
  {
   "attachments": {
    "227da97f-e1ae-4252-b577-03a873a321e9.jpg": {
     "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAE4AQUDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3qaVYYHlbO1FLHAycCuT0/wARa9f2Vrq8emWsul3W1kihnLXCxsRhiMbSRnJUHjnk4rq5zItvIYkDyBSVVjgE9hntXm8ssJhQaBpmqaV4geZS9rHFKsCtuG8vx5RTGfmHJ7c0AdzJ4j0eLUl06TUrVL0kKIDKA2T0GPU+lNfxLosd6tk+p2q3LOUERlG7cO2PWuPuSY9A1Tw8+m3cmq3NxMY3FsxjdnkLJN5mNoCgqSScjbj0p82ktJ4Y8SRSWLPJcaqXwYjmRd8eGHHIwDz7UAdfaeItHv4J57TUrWaK35mdJQQg65J7DHeqN3420K30e71OLUYLiG1HziKQFs9hz3OOPWub8ZaPeXmsTmyt5TGLGBpPKjDeYsdwGKAEbWO3OFPXp3qG5tptVtdZuLeXVL6X+y5IA89iLcMSQQgGxWdhg+wz70AdpH4gtHlMn2q0+xeTHIs3ncne5UZGMAEgAHPJyO1Rnxh4dFobo6zZeQH8sv5wwGxnH5c/Sue1qP8Ati7nnitJ5LaeHTwA8DLuAuyWBUjPA5IPatWHT0/4TzU7lrQeW+mQR+YU4Y75dy578bcj6UAat94h0jTFha+1G2txMMxmSQDcPUe3I56Ul74i0fTpIo7zUraB5RuQSSAbh6/T36V53p1re6V9nnvJ9RtI59JtYY/IsBcHKIQ0TAoxU5OcHAOfapRYNo9lFFEmq29w+nJDsnshdx3KAuRE4QfKy7scFRgjrigDvtZ1VtNt7SWNFkE93DbnJxgO4XP4Zp1/qq2GoQxzSW0du0MssjyS7WUJjJC45HPJyMcetY2qQXMvh3QkNkYpkvLJpIIwWEWHXcOOw9faqfja2lvdXt7a3TfNNpGopGo/iYiIAfnQB18t/awTQxSzokkwJjVmwWwMnH0HNY91410GDSr/AFCLUbe4jsozJIsUgJ9gOe5GB2rnNXvl8Q6lpIt9J1Ga2jguluFkt3h5aEjy8sByeRnp05qr5V3f6bqNlaJdXsY0eeCJ7uwME0DEALFuwofPsONo55oA7S916JPC82tWRS4jWEyphuGx2zU1z4h0iyv47C61K2hu3xtieQBjnp9M9vWszVZP7S8AXRtoZi0loVWNomV84xjaQDnPtWLdOtnZeINJu9Mu7u81CeZ4RHbM6XCuMJ8+Nq7RhTuIxtz6UAdbfeItH0y6S2vtStredwCqSyBTg8A+wzUUeuxrf6rHdNHBbWCxsZmbAwy7iT6YrlNOJ8PQapYa3p93f3l3sIkitmmW7XyUTZuAwCCrDDYHOe9UrfRNWs7z7ZcrLdQ6fBZm4sthIuGSLDOp/jdCAQOhI9SCAD0a51OysrE3t1cxw2wAJlkbaoB6cmse+8b6DZ6fb332+GW3nuFt1eOQcMSAc88Yzk98VH4qnc6XZTwRMY/tKO8wtTO9uu0kOI+pOcDocZzjiuRWG883U794tRuYBqOn3Pmy2mx5ERhvYRqoPAHpnA+lAHpF3qtjYQRzXl3DBHJnY8jhQcKWPJ9gT+FUW8R2dzYpdaZd2dxGbiOBmafaAWYDGcH5ueB3OPWsjxpNGJ/DE72sk8SamJDGiEtgQSnIXqSOuOvHrWTqKS6vq0+qWFpcrZPc6dEd8DoZXjuNzPtYA4VSBuI7HsKAOyPiTR11H+zm1O1F7nb5BkG7OM4x647daraN4v0jWklNtdx5S5a2ALjLMC2MY9QpI9hWBp09vaaamhXmi3dzfi8LOv2dtkjGXd5/m4246NnOeMYzVVRPaQyF7K7J0/xBJeTKsDkmFy+GTA+cYcEhckYNAHc3WtabZNKt1fQQmIKZPMkC7Q2dpPpnB/I1VfxXoMZtxJq1ohuVDRBpQNyk4B+hPFcbeB9c8Q3N0mnXRs5LjTNjTW7KJFSWQs2CM4Ge49+hFM1C0uLPVvEMN3NqarqMm6JLWwWdbiMxhQm8odpBBGGIAznuaAO7u/EOkWN7HZ3Wo28NzJjbE8gDcnA+mT0qze6jaabaNdXtzHbwLjdJK20DPTk159qsE2nrdW8CX73UttFG1pPZ/aYL8iMKAXUfI3G0ncAMZxXR6+JIjouoS2ksttZ3Be4hiQyMmY2UMFHLbSR0Gec9qAE1zxvpmn6FHqFleWdwZp0t4S0wCb2YA7iMkBc5PGeK6O1lM1rFIzIxdAxKHKnI7e1ef3sUup3c+o2Vjcx2k2padt3wMjSGOXLy7SAQMFRkgfc9MV6KMY4oAKD0NFB6GgBluSbaIk5JQfyopLb/AI9Yf9xf5UUASUYFRXEpht3kWN5CqkhExlvYZIFcVY+Mr28g8PXc1pMn28Tb7eJAxkKqCu3ngdeSR0OaAO6x7UYGK59fF9g0QxDcm7+0G1+x+X+98wLuIxnGNvzZzjHetDTNZt9UWcRpLFLbyeXNDMm142wDyOnIIIIJBzQBoYHpRisGHxbYTyRFYrkWs03kRXhT9075wADnPJGASME9DyKRfFtk08Km3vFt5rg20V00WInkyRgc55IIBIwfWgDewPSlwK5GfxuJ9Liv9M068mhe6igDvGqht0oRsZYEkcj0zj3plt4ukg1DXY7q1vJ47KdeIIQfIjMMbfNzycluBk/pQB2OB6UmB6Vz3ifXZrHwx/aGmq0rTNEsciKrbRIyjdhiAeG498dqRPFVraGe3uUvCbKJXu55I1CxAoGBYjjJHZQee1AHR4qoNMtBqZ1Hys3fl+UJGYnauckAHgZIGcdcDPSsk+L7KJHa7tL20xbPcoJ4seaiDLbcE8gc7Tg+1NXxlZtNBALG/wDOuY/NtYzCAZ0HVl5wMZGd23qPWgDo8D0oxWXaa/Y3eiy6qGeK2hEnneapVozGSHDD1BU1SbxhZQ2k1zd215axx2xugZowN8QxlhgnpkZBweelAHQ4owPSsKLxVZtcLDcQ3NqXiaeJp49olRRliuCeQMHBwcdqbB4usXYfaYLqxR7d7mN7qPYrxrgsRycYBBwcH2oA38D0pMD0rDtvFVlO8QmgurRJommhkuY9iyIoySOeDjnDYOO3Bp1l4ntLy5toDb3Vv9rUtavPHtWYAZ+XnIOOcNg4zxwaANvA9KTA9KyNQ8RQWF3JarZ3l1LFEJpRbxbhGhzgkkjJODwMnjpVT/hM7CWVo7K3u71lto7o/Z4wf3TglW5I64PHX0FAG3cWNtdy20s8Qd7aTzYTkja20rn8mI/GrGB6VgnxZYy/ZxZQ3N8Z7cXSi2jztiPRjkjGecDqcHjis+w1+5vfAuraxHMGliN6YH2YwsbuEyPYKOv40AddijA9KxbrX4NK0Wxvb7zHNwYowIk3FnccDA9TVSTxpaRfaw+n6iJLNd91H5IJhTGQxwcEEZ+6SeDxxQB0uB6UYHpWDdeK7K3eQQ215drFCs8r20W5Y0YZBJJGcjnAycdqjk8ZaeJ7qOGC6uBawLczSRR5VY2TepySM5APA5oA6LAorH1LxNpul+R9okYiaF51aNdw2KBk8epZQMdSwrI1zxcYdF1HyIbmy1GKza5gS5iHzqCBuHJBwSMg8jIyKAOvwPSiqGparBpVtFJMksjyuIo4ok3NI56AD8CcnAGOaxL/AMYNDDEbXTrp5hfR2lxC6qHjLYP97BJDAggkUAdVQehpEO5ASCCR0NKehoAjtv8Aj1h/3F/lRRbf8esP+4v8qKAHt90/SuM0DQdSsl0GK5gjUaYJ4ndZMhwwG1h9fQ9K7SgHIoA4O98L6g3iC81ZIfOU3nmpAly0LSRtBGhwy4wwZOhOCK3NA0uW3S+mmsxaPcsPlNy88hULgF2YnnrwOAMda6CigDgNM8K3lpa2GlTaf50dpJGftcl/KYmVGBUiLdw/A4xtB5yelI3h/W7n7ClzA73FtfRTzXUl+zJKqyBjsjzgcdiBjoM9a9A6UUAcda6DqNr4GsdP8qN721uY7gx+ZgPsnEmA3qQPzq/pumXcTeIJZolQ384liXcCceRGmDj/AGlNdFRQBzF1o95N4GsdKRE+1QpaBwW4/duhbn6KaZf6Bd30XiOIFIzfvE9u7cjKRoBuHpuX8q6qjNAHFa1put+I7YxzafFZ+Rb3AXM4cyyvC0YAwOF+cnJwenFag0q6/t7RLzYvlWllNBLzyGbysY9fuNXQgg9KCQKAOf07S7u20XU7Zo4DNcXV1LEso3RkSOxXcB2wRkVyet6ZqNr4a1crbPp9kumyRtbvdmdDJ8u0xg/dUAN6ZyOOK9MzTXRZFKuoZT1BHBoA4/UNP17Wri2fyIdPexjmaKbzA/mTNE0alQBwg3E889OKzL3wlqOslI3t5bNDaXFvI9xfPctudAAwBJG3I9ifQV6JwooDA9KAOWuLbW9c0+fS72ygsoJLaSCaZZfMLMyFQYwMYHOctg9sd6r6Xot5/aGntdaX5X2Mlmml1CWYFthXMSFsDqeWAwDjHcdlRQByWr6bq1xrNy4hlubWWJVgC3zwJAwB3b1XBbPHPPpx1png3w/f6KHF4kYJ02ztsq2fniVw34ZIrsKKAOH0DSNa8OW9oY7GK6Z7CC2mQThDG8ZfnJGCp39uRjoc1b0/RNSh8C6pplwsP266N4QIz8mZXdlx6D5hXW0UAc9qWlXV1pmiQRqpe0ureWUFuip97HrST6TdSXXiCRVXbe2iRQ/N1YK4OfTlhXRUUAcANB1xYDaTQvcxm0ihg2Xzwx25WIKwZVxu+YE55znHFavhbQbrTZbz7bHHsmtLSEANnJji2sD+NdVRQBwEXgzUJNLv7e5kjaWJY7bTyZG/1EUnmJuI5Un5VJH90Gm6j4b1DVLG9VdL+zym0khia41CSdy7bfu5Yqq8ck8njgV6DRQBgeI7TUrq3szYF2SObdcQRzmF5o9pG1XHQ5KnqM4xmsKPw9qscd5OloodtRtryKBroyMVjChlLt/Fwfb+dd5RQA1CWQFl2kjkelOPSiigBI0EcaoOQoAop1FAFe8JFjOQSCI2wR9K4rwz4i1WDQPDz6hYoLG7hihFy1wWm3lMhnXGMMR/eJ5GfbuJ4zNbyRA4LqVz6ZFYCeGnXw/oultcK39nNAWfZxIIxjpnjNAFSLxZqDW9tqcmlxro11IiQzifMoDnbG7ptwFYkdCSAR74WLxoZBof+hHdqDMtyN//AB6kMIznjn94Qvb1pIfCmoLa2mlTalE2jWkqPHEsJEzKjBo0Z92MAheQMnH1px8HMDrhiuwhvyGtvkz9lbJfI55/eHd27UAWLvVn1Dwrr1zEpiEC3MUUity3lqVLD0+YMPwqlZ+JdRsrfTJNV09ILC7VY45xOXkRiuV8xdoxuwehODgGtaHQTD4PbQ1mBdrVoWmK/edlO58e5JP41nJ4X1G5WwtNU1GG50+xIZESEpJKwUqu9txHGc8AZOPpQBV0/wAeG7uLGSSCySxvpFjh2XqvOm77hePHAJwOCSMjPfE8fi+6XWbe1vLS0hiuZzBGi3ga4Q4JUvHjgHHYnGRRpnhO/wBPaztzeWZsrQjY0dmFuJFAwqs+SOOMkAE47VBY+Cry0OnRNeWZgsJ1mVo7TbNPjP8ArH3HJ55IHJ59qANNvEsh8NaXqq2w338lvGIjJgJ5rKvXHON3pWHp2taxcWVrJqUMTTvrE9tB5N0yrlfOwHwoyo2Y6HPBPPFXE8Jat9jsNOk1WA6fp91FNAq25EjpG4ZUc7scAYyAOQD6g27fwvdQzRA3kTW8GpvqES+UQ3z+YWQnODzJwcdBQBmeEdRv4bOxe+hVp9VvplMounk+6sjZwwAX7m0AcYwataprE0/irTrBd0a22qJGzK5/eK1rK+CPTOPyqz/wjN5BpGmwWd7El7p9w88UskRZG3bwVZQQcbZCOD1FRW/hO+/thNTvNRjmm+3JduEh2r8sDxbF5OB8wOTnp70AVfGes32lzX7aapF3DpZnV3nIRQJMH5MEFvfHtXST319aaI11LZrLeJHk28EmQW9AzAce5FZ+u+GG1q5vZPtQiFzprWQ+TO0ltwbrz9Kn1bRrrWfDbadc3MK3LbGaRIiY2ZWDYKE8qcYIzyCaAObvvFFzf6df2cptI7uB7WUSWF35yFHnVSCcAg8EEY6GtXS7l/8AhG9emleWTyru9x+8IIVXbADdRx0x0qufB17O9xLPd2iPNHBGI7a28uNBHKJOBkkk9OT/ACrYttCeDR9UsTOCb2W4kD7fueaSce+M0AYUHi27XybOxsYpmhtIZmiuLzbPMHQH92CDvx0ySMniu2jbdGrYIyOhrjb/AMG395py6c17ZS2vkRxA3Nnve3ZUClojuGCcZGc4PftXUWUF3A0qzzpLF8ohAQhlUKAdxz8xzk5460AXKKKKACiiigAooooAKKKKACiiigAooooAKKKKAFooooASiobu5isrOa6ncJDChkdj2UDJNfM3iL47eJr3Vpjo00djYBsQp5Su7D1YnPJ9BSbGlc+n6K+SR8ZvHGMHVgf+2Cf4U8fGjxtj/kJr/wB+VpXK5GfWdFfJw+NfjQD/AI/oz9YhTx8bPGeP+P2P/v0KOYORn1dRXymvxu8XdWukP/AAP6VIvxt8Un783/fOB/Q0ubyD2bPqiivlkfGvxFxmabP/AF0X/wCJp6/GbX2PzXV2P92SP+qUc/kP2bPqOivl3/hcWvEH/TtQB7YeI/8AtOl/4W/rh/5impL9PJP/ALJR7RB7Nn1DRXzCPi7rOedY1X/vmD/4ik/4W3r27/kO6kF9PIgP/stL2iD2TPp+ivmJfi5ruefEOofjawf4Un/C3fEAbI8RXeP9qzio9og9kz6eor5i/wCFueIiw/4qWYL3zYJmkPxa8SdvErD62K0e0QeyZ9PUV8wf8Lb8TY/5GRev/PmKT/hbfibH/Iyp/wCAdHtA9kz6gor5gPxZ8THp4kix/wBep/wpB8WfFBBz4ii6/wDPuf8A4mj2nkP2T7n1BRXy/wD8LY8UY/5GKLP/AFw/+xpf+Fr+KC2P+Ejh/wC/P/2NHtPIPZPufT9FfL3/AAtbxSP+ZjiP/bL/AOxpG+LHisYx4gjPPOEH/wARR7QXs2fUVFfLx+K/in/oYE/75H/xFH/C1/FWM/2+v5L/APEUvarsHs2fUNFfMMPxS8WzuEXXlyeOSg/mldHYeJPG9+2xddkt58bkWWOMq/0IXOPfBFTKvGO6H7JnvdFeN+H/AInarpus/wBneK8FGIUTqgwvbPy8Ee9exqQwBHetIVFNXREouL1HUUUVZJgeNo3m8D65FF997GZV+pQ18jf8IhqpA2xqw68OOn519ceOCV8C66w6ixmP/jpr5MW8mH/LQ5+tYVZSi1Y2pJdSP/hDda3YFo2fr1p//CEa/tB/s+Yg+ik077fMOfNY/Q1OmpzDjz5MnvuIxWPtahtyIrf8IT4gxxp05+kZpjeDNeQZOnzge6Gti1u9QupBHbzXUjf7JNa9nZa7MzA3FxDtbBMkhXnH61EsRKO5UaV9kca3hLXEIDafOCf9g03/AIRfWs4+wT/XYa7tbTxKVbZc3TAHBAl5I9uarPq2s2r4mvbqNuhLs36H8KFiXLawOlbc4s+GtXB/48peP9k0HQ9SQHNpJx7V2n/CQ6rE5X7dOCB0JI/rUw8RakMf6Y59y2cU3Xn2D2aRw39j6iFz9kk/KkbSb5DhrZwfcV3Z8T6iGz9qLZ6FutIfFGoZ5lRs+oGBUe3l2H7NHBf2feDJ+zvj/dpDY3Q/5YSf9813TeJL0nO+PJ4yUFM/4SO4GPkgOP8ApkP8KPby7C9mjhfs05yfKfj2pGgmA5jb06V3P/CSTONrW9uD/eEYz/KoW14hvmtoGBPUxj8ulUqz7D9mjiPJlPzeW2PXFL5chP3DzXX/ANsYZtttAvf7g703+2QQS9pbdeAEFP2z7B7NdzkPLfP3T+VJtb+61dcdWQ4Js4T6YUUw6rFuJ+xxH/gNCrPsL2ce5ymGxyp49qMMR0NdX/acG3mzgwPbrR/aNvkg2UO3H92n7Z9hezXc5Xa3PymkwR1BFdUNRtMDNmn4CmG9tMANbIc8mj2z7A6a7nNYPYZo2t3U10YubVQMWqADp1pDdwg/LAOmKPavsHs13OdKnGSCKQZ9DXQC7hJO6ADPpUy3VmeGtl56nrR7XyFyeZz0QBJyK1LPXbrTsRRTO0G7cYs8A+q+h9xXSzw6DqWlOttH9nu4+d2eHH07VwcyGOZ0PBBpwkp9BNcp27ag2r2yLI29wMpI3WvquzJNlBnr5a/yr4102Qraj5j8hIr7Jsv+PG3/AOua/wAquhHlbRlVd7FiiiiuoxOe8df8iHr3/XhN/wCgGvkMbvU19e+OBnwLrv8A14zf+gGvkrHG3t1xXPWdmjejsyoQQ+CpIrrNA8KDUoRd3cjxwOcRov3iPXNYMUal13/czyfQV6bZ3dimnobRkFsvyIc4x/n3rixFWUY+6dtGCk9SxaaZb6XaeVbBVTPTux9avJBhwXPDDPXpVRbiEhy6lvl+XByW9xTra6uJlCvG6HAyWH6CvLlzPWR2pJKxpxxrG+0SbVxxn1pGtklBRtpXIZlYZrMmn8p0LMVVGwS3H0P54q59pjDKJZirv0TFZ6g0RanZRsxhYb1nUjGO9cXP4a1S2P7u1eWJzhCgyfxHWu8E6gEkncFBG4cAep9D1qEaskUsRZ8I+RHj2/pWtKtOGi1M5Uro831LSb6yiSS6t5YkccFv5e30rMLYOSTx7V0/ijxU2pQmyiGYA4JZhjJHpXIGd2cADj0r06SlKN5KxySSTsiwSMdOtMOSuSfrml6AfSkC/KMYwKuxKRLZafd6hOYrSF5nxnCjoPU/nWong3Xmm8r7LsJXeCzj8uvWu88LeHDotqZJmja4mwSyc4XsOa35MMQqs2VXcTj0rhq4zllyxVzojR01PBruCa0upLeZCJY2KsvvRciCKRBBM8o2jcWTbhu4HPSvTNe8K2+sB7hJNl8R8pUcMMYAP4DrXm32R4rhBcW84CtmRShBC8d/zrto1Y1I3RnOm4sr7znHqKfDBcXT7LeKSVu4Rc4rYutIGoX0S6VaNFAVGZHbK59zz6Gu28PaGdL02PeqGZyS7rj5s9OvOMVNavGnG/UcKTk9djza80y/so1kuLSSJG5DFeKqpuOAoJb2717h/ZEFzbut1teOQYKHkVx+v+BBFmbSsg55QvgD6d6ypYuM/dlownRtsefE5HHGaAW9M1fMH9k6iI7y2LyxN88TEbSOPzGM/pWnp+g392VurWCPZMrH94nyKD2GeSR9K63JJXM1Fs59JWjdWBAYHjijfXSt4E1NWjy0R3/eJPC/41SvfDOpWQXFv5yDq0IJ/SpVSnLS4OEuxkq4J6jnrzU8aoVPPfjio5beSFx5sTxkjI3KR/OlRyBjPbAoa7GdrE8ZKSYUHp29KyNRU+eW5rVgZNpaRN/ykDBxg+tVdShJi3+neqp6SFLVDdLbIlU+mRX2lZf8eNv/ANc1/lXxLpr7blck4YFa+2rL/jxt/wDrmv8AIV0017zMKmyLFFFFbGRgeNhnwPrg/wCnGX/0E18oeWN3II4719YeNRnwRrf/AF5S/wDoBr5TOT1YseByT0rkxO6OijsxoUA8Vs6R4Xk1aISzXJitmbaUU8tj+VZUUe6VVBOSwxx0r0/S9NjsohBEp8temerH1NcNeq6a03O2hBS3GQWVvZKttaxoiRqAuzqPxpxlZHZZCwZTjKjrxVmRYvNeM4UMPvAY59qSMwEskDBmHXHJ/OvOcnJ3Z3pKwLwWBXc7jBwcZ+ppup2DX1gYUmMUicxyDnDdRn2pyEx/KWwCDuboVNSR3DKiPIVVm656GpvZpiaOBm8T3VhDLp2pRyRXCqRubgH06DkVkXnisTWlva2iTrt5djhQCTk7QPrWz8SpUuIbYRsitDksBzkHGOfz4rg7NskK2D79xXtUKdOVNTSOCrUqKXLc3VtzPFl2YA85ApHtkRFUA7sfNkjn3FS2c3lJtbOAOD/n2qWVN/zBgOd3Bp3adiNCpFC8jqFG5s4wPWvRbT4c2zwQtPezCQpl1UKAD7fSsbwRZW8+oyzSMpkt1Dqp9fX9K9KR8R4xg9vpXBisRKMuWJvCCtc52912LQDFBJE8kaYR3Ucr2BP14rTtLqLUo2mi8xG9CMcVLNp6STOZHD7lPyMoIzx1Hfp3qtqkMum+Gm8navlwkwu7ccZwT09q5OVTStubuVjQiUrB91ducAjggYFY+tWMF9pdxbupaSQHyyWIAbHBrI0LWtXvPDa3Umn7pRuYvIdu5eeVHU1W0rxC+r6mlpGC0axs0kgUgD0HNaRw9SnJtdBKUXv1L2laOLGzS1yWKDIb17n/APVWg/mRR4IJ2DAJHtTwksUiiJiep6cY9/8APeril7tCFjOOQRjk1hKblK7NrJGbHqUojwxAIOMcVfSWK8gdcM/l4OPU1n39lHbxNNGAoA4GMnPaoTfJBBHIr9cg7TxxV2vsJpNFTU9JsLzXrSa4X54gQQOjkHgH1roEMWMAqMD8q841S/1TUdZt/wCygXVHJZgDsHsT06A11NnNqSNsCBY8Ak9ef8966KtKahG7M4STbSR0IlfyGTaoA5+tVXg8yM88Hrmqq3d1u+Zfmx1PTP8AWkN3MZNu49OwzXMk0aJWFutMgvLcxzxLIh7N1zXmuuaU2k6g0QDGF/mQkdB6V6azkkkMwA9f8KqajZC+06WB+A64+npXTQrODs9jCrTUkeW7sjC4HueAKLxmltCznLdzTrm3e2uHgkI3xnBIHBp5jLW7JkMSM16SeqscL7MwY38t1YcYOa+47E5sLf8A65L/ACFfDTqVcjHQ19yWH/IPtv8Arkv8hXbA5ahZooorQzMLxn/yJWtf9eUv/oJr5UfCrnn+dfVnjLnwXrQzj/Q5f/QTXy1LGYok3MuWGRg84z3rkxG6OigtGaPhrTPt1+JJCyRW2JCRzuOeBXoqZ5KN0/SvNdK1aTSLgnbmF+HQd/ce9dxpOuafqMZFvIPNVjuifAY49vSvKxUZvXoelQlFK3U0MxzZIKMV698URRJEhdjszkDJx3przRRs0oPy4xhRz9BUknltIv7wM4IwA3K1xo6WxbhYkXa4+U84PUiqkJk1IG2iRCsBBaVz9/H+cfhTNQnjVGhkkImk6c8gdc/TjH41hR+LLLSbt4VcyLIBsRcHyzkls+o/OtI0pT+FEuaS1Mnxxpk0U5wENuwGCFA28/8A18fjXPWWnRooYEk8ZJ7DjpXq9ytvrmkyrKsTI0ZzJGwxj2P4V5iZIrTUriCGUzQJIfLdjXpYWo5U+XqjjrR9/mJIrMPISNu3buGTgcVqaVpy32rQ2jghOS5zztA5x+VZ4RQSNwJzgcYzW14UdU8QQDcVDBgPrgkD9KdVtRdhRs2do+l2+n2zTafaxQyoDnanzOB/DV+2Y3CecisCQBtbtx0x61S1Oaa3X7VFGWeP5TEOrUkF3HavC4G55pAEDnkAjluvpXj+9PVnbay0Ld3i4tZWL7dilCw4x6kVUhlfVLBN+1l8zbIegYDBJA9/0qXUZftsfkxyFIV5Yjgsc9Pp61LA6rbbD8oQZUYwDz/WleyFay1HXsKmEC1UIgADBOMr3x/Sqd3Nbw3LspiBWIMHGAWXnqfb+tasVwWupF8nA2ghs/Ln2/WuR8QeB49Zv2ujqM8T43JHsBQe3H0rSi03acrIl3WyNmO7Sa0JzhzgjI6jH/66bHqJhikWAMzk8kdB+NVtA0nVbBTBeXlvd24G2JVX5gB+HTms3Xb+50m9dJoVitZMmBkfnI6jnoTn9KqNG8nGLuV7RdR+p3DSWU63TtHlcLz36j8f/rVl6d4Xu7lvM1e9mFuxylrE2MD/AGv61e0Rf7YK3l1F8kUh8tXOQCDjd7nr+VdJENw343DpwK0dR0FyLcOVTd+gWljbJCgEarGq4VVGAB6UnyRyeWijB9sUx0KuSh3EjpTXhIjBZ1Xgda5nJvVl2SAmMkgge3fP0qvKIyVwh3DJyOn5VcIhaIurZPY561T8tlUtkYOFwoyeefwpodyOKbeApJ3j1U+9PZXlAHRT1OKaitDE5kIL55cDHH0pba6Q7o2Ab+L+dXbS5DOZ8W6RFLD9sg2JJGCW3cZUD+dcdayrIGRSrE98V6RqUFtdWc0MzHEkZGB2rza2tDCPMWNljZtu4r1x1r0sPK8Nehx1Y2lcx7wbbphgjmvt+w/5B1t/1yX+Qr4o1aMrc7sYya+17D/kH23/AFyX+Qr0qTvE8+ruWaKKK1MjD8YAHwbrIPT7HL/6Ca+WZYAdwUA7hk19TeMP+RN1n/rzl/8AQTXzI4AB+X5iK48S7SR00NmZwO7achkxk+v41H5ht7pLi2ZklU8HHGfx7dal8kRTrJFyFbLqeh9qdM3mMW2KoLcDBwtYp2OhaHonhy7XVLCKVsBxlZMcYYdavtZqJCwQI3J3jqOev1rkPCl4iQtA0gSUuWGRgHgdM/Q1213KoAkCjdsHNeXXjyzdjuhK6Rx3irTJb6ZJjdyC4+YRrtAHsCK4jM1xdnz9y3kI2YIAx7ACu/1ScSzWrK3zIxdmxwBXF3BaTxRcvgAgDcB3O0V3YWpLkafQxrwW4kUt3HBLbRzyJFJw6BjtYU+G1SBdy5xjvzWu25rURHAQHOMDr9armNRxkHnnFX7RnOyKIkYY7GVuAM42+h47c1c0y9/s3UobpV3mM5Iz1GMGq6qAxIAz+VRtlSRkbT1wKNJaDUrG/wCJPHL3Nk9tYWcokc4LPg7gRz3qLTtdgl1PTolSS3gjxveYhjnbgAHsMkflWC9uCAxHQ4GB0P0+lOaFpIh5KliTgKx5oVKmo8sUaRqS7nq7eWqgA8nJ49T3/Oq6TJKHxJjZy2ePqBXAWnirUrM7XcXSMFADnkL6ZH1qhLqOpXV47iV4UkUAJE3fPXNcccDK75nobOvGx7FATGfMyGU+vQdutI0hd5ZMSFUxgleGJzk1QsUvI9HjhvQn2gLhzu3HPY5/WteNHuYUd87WyTk44ya4JRs2i79SrblLZXkJ3LyVBPPPJ/pXmXxCv21W9s0S3lijwxV34EnQDH8/xr0u7kWOYIygozBAO3v+grmPEVnb6xpx+0OUFvNttlRcZI4K/j09uK6sHUUJ8zFODkjM8Eq9tCbaRmKsd8YI5HHpXdsyQQ7l6nisC20dLNFuI4isoUAMWJKjH/1qo+IbjW4bZJbWGOWLdmRRksf/AK1FRe2q+6Ne7HU1/tfmyMwOR+mKc11GFXdlu2Txj2rkW1nWDbq0drH5jANGhY8A/h1q1pN9dXwKXyqt0jkiNecKOhz/AFpyw7iuZl86bsdJhDEA4ZWByQT2pVIkclVPHXd3pjyZURAAc5OR2+tLuAddj5wOnc1jYuxDdOiLmRsHoBjP4Vz2ra2ukW/2h0eQuQqquB26fpW5eSwxxNvbB64J4+teWavef2pqjssjvbxn92pGAPwrtwtJT1lsjlrVORablq88R6lqHmIpW3hddpVRkkfX1q5Y3FqtvFGs7rLjD7x8v/66w+wHTA6VbggE42ltpDDIHJx6gd+ld0oq1lojmjN812HiSVbqVZ1B24wM+1fYlh/yD7b/AK5L/IV8cagoksj5RLrGThiMEj6dq+x7D/kH23/XJf5CujD/AAnJX+Is0UUV0GBh+MTjwbrP/XnL/wCgmvmNnwR8oJYd+ea+nPGOP+EN1nPT7HL/AOgmvmDz5G3dAucL61x4ndHVh9mNcOnmKeD9c1Xc9GTv19KsyFXlKlWIzjg/dpjKo29FBPeua/U6SmryRShxlZA3GDjFegaVqN9rWnyyR2uDGNrMX+UnHb/CuClUpIHGfx716f4UiEHhy3VwT5m52xzwTx+gFY4nl5E7amtFtM4/VJl06xckkMgO7cu3c3PHv6VgaXbukCTSMrPP+8Zicnr0P5V6vfWIvtLltJfmRweH5GD/ACFclaeHrGPSZ4gH+1QyZD7z8y+w/Me+KmlXgqbXUqpCU5XMtVIUD72elWJbOSKLfIoQA45IBb+uKkex+1Wjy2hCyQ/fhfPz49PQ0y0vzf6eYSV8qNgxQgBhjrtqr3VzJwtoUvKJzgfiaeYgY9rcn37VKVycA8dvelCkcj8TTMiu8PlrtAyDySTkjrxSRrmIcE7iBtHPt/n61ZbOM8/yqLCjZHu2k9GzxVqTKVyvcRTRARNE0ZxndtwSD0/kafBbsclQCFKhSAQAcjAH4Z9utaNvomo6lEZkKSoAVBkbJPXgZ/Hr61t2Xh64ijWK6XajnayQn7464Y+3tROvCKtc0jBt3JdM8XJvlh1JUikRfkkUEBwBzkeuK0P+EhQ6e00U6LAp4ld/4fpWVeeEEluIpLYtHF8pkt35JGecNnirp8JaM9sFitSdozyTn8eetcU1h783c2XNsc/deLA237Ej3M7IwSaXICk8Dg/XoK0vDWk3dnbm4v7h57mUbirnIj+lJe+FYrfZc2rmMIeUkG4bgeOevatazvRcRKJBscZBU+v9R71dScfZ2pLTqOEZOV5GgoLBA3GOMjnJpZYWjjYP1qrEzrK+CR2AxzmpvKZ48Mxbvk1xp6mrViq1lHhJYUEbg5IxkE/0rJmh8qVp0cKynD7lzwff0rdll8hgPmKjPUdBXP6lc774iN+DHkn1HpW0Lth1Lq3HnDEbY+X5sc/lUMl95UTtJJHGoHzMxxiuHtb+/VL2exuhHbK52eYQcewz+J/Cse/u7i5YC5uXmRjnB4Gcegrsjgve1ZhLE6aI19e199QLxWTO0YOHnwcY9BWF5agEICFPQen409WYRlFbEZOSvrQGLPjgcY4GK7oxUI8sdjlk3J3YiuoXkH656U43EsIZVldVb+4SOaaiBycduenpUyx+Yys5KoGGXC5C/hTvYmwiyiW3liVSo25Abn9a+zLD/kH23/XJf5Cvj4QbJPOlcbJASDwMjp0HSvsKy/48bf8A65r/ACrWi1rYwr9CxRRRW5zmF4xOPBmsnGf9Dl4/4Ca+YRtBBVSDzz1/z2r6e8Y8+DdZB/585f8A0E18wyxoxj8tZMY+8eme9ceJ+JHXhtmTyy+ZKrMqnagUBRjJHc+9QfIwZmyzk9cDNAVlJK45Ppn8M0sgVBuJAY9s9xXIdO5BJEX3ZIUnrnqK7jwxrUL2UGnSuftEQKKSOGUdMfh/KuM+Y4LEZ5yeorZ8MRxya2zyBsohMfOBkYH48Z4rOqrw1Lp6M9BSDDMBIfmAx7fSqUkEccUi4xvPOexxj+VXYXPIyeewFVpVkVnH3o1OM+przb6nWlqUIfs8FvLiNdrAbgAMk1hJ4atI7JpY3lSZySfLkJU85xg9u1bk43qEXGSOf502VTHZtlvm2jafTmtI1JR2CVNPc4Ke8ezvmtblfmXjd3I7HFaCgMoIxnvR44QKsFzsI3EBT09c1W09zJaI3Ukf55r0VrFSOCcbSsTSAkEAHB6nFFpAst9bQvwJHVSQeQc4/rikZ8NtC5HuePWpHdY5YZQrKQwJOT83uPxH60ndKw46Ho1tZ28KiCOIRjkhe1T+Sq4EqlsH16VDBcQSos8ZDFuRg9Panh2YnkjnIOOleS276nZuhrnaWYgKfr1PpUcU/wDpBjYbTJzlRnmpnldUYsmS3B9qqiNgA7qrSqxXg8/e4/pQkNLQsyBWSRVcsSxLqQByay7rSIWl8yKeRGj+bCEYPrxWosSvcK7sOR8wI4P+f61CY13scdyM1UJOOqFoc/8Abrq3vIWuLeQW7nC3AcbeRxkdRzWomtxeaI3BDk8elXmQRq5aXfzgAdvavMfFmrFdRW2sGGZVIkIPK88EY6HFdNOmq7slYmVRQV5anfvc21+rRktjlSR1H5VlXsMS2csTKyqoOGHXGK5rQL9rCzEYlEk28s5c8kH3rW1TUIrvTiwn8l2BHqy5yM9frVKk4TSWw0043OHslQwYDlYgxBPcjscfnVd1dBg4K5x7/nVi3QAm3R1ZkJy3Y0k0fyg52mvTTtI4CunOeOnXnpTlXk9SfamKpJyxXjuRin54PPTvVNgNDmOQ7CVyMcHH4VOkzJEyDBQ9QRmogmQSWG7PBpSzZwRjFJ6ibAy72y7d6+zbH/jwt/8Armv8hXxhwQTx19a+0LH/AI8Lf/rkv8hW1Dqc9boWKKKK6DAxPF4z4P1getnL/wCgmvmV/kVQh68AV9NeLgT4Q1fBwfskv/oJr5q8o7yOOOeetcOL3R14bZjFVXLHI4PPH6UkiNG29lZOPlyvB96V1KMS3I6gCml5rh8TO7rjnJ+6PQVyo6iJiSSVJb5e2K1dFufsuqW7kYU/Jz2B4/nWW8CKpRHKnaOe9S7G4aNWCq3HsacrNWGnZ3PTRMAAckADkij7sfAODzmqOhzJqOkiSYnzEbY3OM8VoTFjgxgYxwprypRalY7IyTKSojSEkZduPpVW9VFU5VmjGcsOvTGBS21x/wATN47gMjbDtUHjnHPv0qO/njRWEe5ij4UE8kkZyPzqoxakOTseb+JpLwzWP2ucyKwJ8rP3SDjtWpAAsCIq7Qqg7RWRqEs2s+I3ilUIlvlcL046n863I4iF+U7QB0P5CvXqK0Ipnn3vJsjACSF+ckDqfTPb8acHVYsTFwd2VYEEf56elNkQmM87iBnPHJ/z6UqlInAlOWDZC9m+o9qi2paNvQLuCxvXEtyQAmUJf5ckjPHr/hXVWd8LpUZGSQbiCy9DgkV5vEQTmQDYc9D0yCee4/lXT+F7pGhNqkbryxRxyvbj681y4iimuZGsJ9Gdk7LLA0i9M4wD/nvVfcRGRIqmRsHeD0pghmVE2MVUdifvevFTyo0duWIy3pXCkzXYihi8yPmTcQevfpVHUZWtI3+6OAwcn86stNDbEB93Jxkc9fauS8Xa9FZWb28OZLicFYx746/rWtKm5zUUKc+VXZd13xCmm2s6FzLPMf3KjqxAx/QV57Y2xPmTzkNM7HdUlslxKFkvJGmlUcFznA9B+VWQFBGARx+lepGCpLlicUpubuQSDAUBugwM/wCfrUDRKwB+br2brVuQDdjjdnp61E43IOnoQKabE2QpHHCuEXBPeh/uHDEYpcsoCnkDpk1Gwyc849BVLViIyN8RAHBOBkcZqLywrbSB7dKeQdu4jGGwMc8U8cYOTn0PaqegEaggH+dNLHd8zY4qTc3Yc5xTD8x5/WgljCeMjNfaNj/x4W//AFzX+Qr4ulcYAWNVwP4c5NfaNj/x4W//AFyX+QrqomFboWaKKK2MDF8W8+EdWGf+XST/ANBNfNTxrjnq3XvX0p4t/wCRR1f/AK9Jf/QTXzakYYBhuJHT6Vw4t6o7MNsyAoCE2thPp1pnmKrEIJAT8xwOtSNlJnYDKjAJPrSKh5PK8cjrzXOjpAJncUznOf8A9VI6jzBnOQ3HU9sYp658s4Vmzjkdff8ApRFA7M77FJbJ4A/z0oWwzu7aNdO0i2tmL7iyq7DnBb3H1/StW5iPkhQ3lgrtB9KxdM1KzuLK2tZJVeUYWRGHORyPr0FaV6fOi2q5UjnPTFebNe9ZnTHZHM30e3UY1aSTbEeHHYngAn0rnL3xJd/2jNpdqseThXuU+cjHXHp2rpLqcxFVKO2ZdgLHgnIOfyBribG1+x61fJETIijqR3P047/pXfhoxs21sZYiUtEmW7TTVgOQzGRzuYsQd5znn0zVyRDsChSVx93PXpT1jkiKEjAAwMDmnyoJoflIYnt68Z7VpOXMYJWKwQlT5bbs9Pm/Aj3/AMailjV/LBwWXgdRyPX04yak84xuBhTg5L57cin7+V5TeoySBzu/OqQ07FdbYmQzSOpA2oqrkLjHHPc//XrqvB7IrXZRGWLIAH3sMOPwOAK5lWBOTlj1BHWup8ITjzrqCV1VhiRc4+b1P51lXv7NlwacjrIbhngj2DkfMQRyM/5NQNI7tJG+QyYPoDnNSrdQxIE8wB3yBjnuar+WVMhIZQW5J4J968w3iYeuzvbBQi735KLnGT9TXndtN9tuTd6kfPkO5NjkgKOmB713/i5pDoV2Vkx5ak5PQHj/AOtXm2lsxsyN3zZyAeh65r1cGkqbZz13eaTLsYYIfmwAMZ3H1/z+VSb2IIz1+6f1pYJvJm3yRqyY6N0I6U2J8rypjbjhj1+vp9K3au7syYm/C4GScYyTzULMpfoSc4qV8lcrn1OM8/8A1qTyi2AeAfy5qZWuCVyBgxJIIHPUDJqIlt+Mnnq2K318Oai6M0MSyR4+V1bAb6ZqjeaTqFkga4tZBHjJdRuA+uKUZp9QcWjObClRt+tKDvGCpBzSearZZCD64pwddoOQDVbIkif5W4ximE4yBnFSMwIIJBNQAnAweM8U0JjXHJNfadj/AMeFv/1yX+Qr4sf+L096+07H/kH23/XJf5Cuiic9XoWaKKK3MTF8WnHhLVjx/wAeknX/AHTXzef9W75wUJPB619HeMP+RN1jHX7HL/6Ca+akcMqIM89C64Un+lcOLWqOzDbMSRCSsit2JBByRipbaNnRTJNs+Un5iB0PPFVVYlmRQGCnkgY61YQJMgONrHqP89K59bHUEZExXoR3xx19vx/lVloTDCpZlYyA7cdM5qMRoqhncEtwvGcc9/ShlkAAUjC5JCgEn3oBm54dtgrPNJhmTKKAM4z/AJxWnfXq/Zg6nABw3tXM2Wr3GnxS8b94G8e+e1b6JDNZLMP9TOARnjbkYx+dctaD5uY6KTWxgX2otDpsrlRueYmLjkkjC/nms6xtFsoFQkvIp3SZz8x7mmXlyst7BYpjAIkJ6jCnA/M1eCkxpygLD5x/d545P1rrhHlhbuYVHeRGsyvnfy4xg88H3oU+bEDuLcYB6c9Pc9/TvQ8QLcEEDn3FMKkTA9COMjvzxn+g96pJGYsiqclHByeOPpj8qbjcSuCA33gOCenPb+dLwMsDnB4z1/EfgajdgWHysSeOP4fTPNO/YVxHR1LHbt75GB9ef8mprG8ewukmjUsE3bwT1GM44qpu/wBgb+OWHB6fpUyushDLG4Y5BZlzzz/nkU7JqzHqtUeiaddQ39kkjxBPMGcdcY96uQncfKY/L2PU1w3h/Vvsdy1vO5WGRt2/ONnTqfQ8V26zpIuAxPuDXl1qfJKy2OqMuZHGeOYHXR5rmJypQ9N2QVPYj8a4mzO2xjIwMdx64yfrXZfEC5hOni0jcNNMy4jVumOpPtXKwReTAschzgAZ7D8fwr0sLdUVc5qrvIkCAswU5HVt3Tp/9emhgMEkEA4JPYninlwYwrMoweKcy+WhDn5sjG7J+latmZHkAHnoMc9qv6Hpb3s5kl+eGJxuUnlvYfhVDBzkjBP6V2XgyHNhI2zbmXgnuMD+Vc9aXLBs0pq7N/euzAwM9ABx9KiaQsjIACD61Lc24VD1H061Sgwgcc5Pb0Neet7nbZWMTUfDmn3dy0jpslK/K0Zxz79qxLjwvcQnbDcq3++MDH1HeuymTzn3ZAYcnBzmoihz1U7s/hW8a81o2ZSpRZ57eaRf2UHnywExd2jO7b9azEfrz8p716bKpU+W2DGwO9c5FcBrOlPpUyunNrKx2jByntXZRqKas9zmqU3FXRQkO5T7d6+1LH/kH23/AFyX+Qr4okOVz2r7XsP+Qfbf9cl/kK7KJw1ehZooorcyMTxcSvhHViMZFpJ1/wB0182CKdoyvmFjnJJGSK+lPFhH/CJatkZH2STj1+U186D5NwQBVzkE1w4t6o7MNsyk1s7OW2/M38PPNXLC13TNE7CIhjkngE96ZeKfKd2YgqPvLyT9P1p8Ee+EFsn1B4P4j8a5dbXOkSSHdMxBJO8jj0HtS/M7nOdqkEEHvVlLaMkYbnGTnp70PEM8cbTjk9cj/wCv+lMGyg0TgEFBFg4UhgQc/wCfTtWtp1+bWya2kaSSNQNgP8PPT6ZqusW5y6hiAc8dB/8AXpTggkBf7wAPf8KJWkrMFJp6GY9o8mtS37DaQuEQLwBnnPv1q4FjZiF5HQY9PqKsopH3h83YfWkIVAO/HXHP407t6E7srvEwKKmMnrycn/OaiLMuFJ+b1bqv9OtWpHw24jDA9un4flWW2rW8dwYjkgDkjt7VUbsHpuW0Q5PzZOBkN3/+tzTJVbewRVLY64z0/wD11KJw9sHXjPX5vy5/Cqu8RE5kJ6tnk59qLAMIIlxlVGNwBOen6UmAWVEaQFuMheCAec4+n60nmKcbc5Y4IA9vTn3qpczQRonnKPOVgwVuCMVcdWD2LSj5ZE8xW5JIbnAH+Qenf3poubkgH7RIoHGEkIA7cVBFeRSkAbmX7gLNyeP61YJBYxrCCEUNgA8A+vv/AI1TWok30EIEp824mLtk/vGbOfYnt6VGYxvKgBiWJ56Funv71J/sbgG/iUcjtRFmOUFgjKcqS/rjjH05ovYLFcRiNicct94jqSPakbI3MoO4cYJ6881YkddyoEClcjf6jqPy/rUbJkE4+X1OKlyFYjjwkoLZxkbiBk4+lek2VzD9nR7fZ5W3CbOlechSo4yc881c0rU30+4Cjcbdvvp6e4Hasa0OdaGlOVnY9GdzIgJbn2qoY1AYqmQBzkVVhnMiCSOUMDyR60szyRlXDDleQa4bWO1bDAWRtyrhuvHQil8ve3mk4HXk0+CVZpNqfM2fTpTdRlEIRVKg4wQCKES2Z0nySyEEurfoao6hCj2zwTJ8jrUsl0CGAI29SoHSorieM2o3DBHAArSLadxStaxwV1avZnY+CpJCsDX2pY/8g+2/65L/ACFfGmsyb73y1GAgHPqa+y7H/kH23/XJf5Cvaou6uzx66SlZFmiiitzAxvFQz4U1UetrJ/6Ca+fvLGzAxnr82cV9BeKTjwrqhxn/AEWTj1+U18+knyySAnGe1cGM+JHZhtmVnRmk2gYPoxwPrxVi3Tyl8tJC2eckjv2pV2sh25bAx14qK3iYMwwSHOOmD9K5Oh0lzbvU85OPTpTCuPlT+LgkHjP+PSpCoPpnOMdM0zYoj+ZicnHJ600Qxgiwcq2TxnPNCbT8ykN68VJ178Y7N+lW7SyWVmMgYKuMnPX/ADiiUuXUqKctEUI45PJ3yAcfexnHX3qkLndqcOnJCdzn5mwAFHr710lzaRyxqqjCrwCe3FVZIcalZrEigbWJkxyfbPWslX1ZuqK0FnsoTbiHnHTO3ufwrJHhbTxBsAZnYbgxPI9veuo2EzAMnyjrk96k8geQzD6nJPJrJVZLZmsoRe5zNt4Us7ezJlaSZ2x828gIfb/69UIPDsJuHWaWZlXhVZsD8x7V1zQE24GWOOiqKje35B2lQDzzzjuK09vPuSqUOxDpum2WnJshRVjwMseSx+vt+lUvEnh611aDzQpScfKJVHX2PqK27K1EoEbDIU/LjjH+NT3UUocq8jADGAP61CqTUuZPUTjF+7Y8Xu7S70a6Ec2ATyCCcMK1rK5NxCCXBwRwW561p+M7aEwrNKhLhSFYdjxj+tcxo6tI5A3Ebccc+lenCoqkFI45R5J2R0GVkRTxgZHTP/66iJDqCycdvUHt2pEjMSBYxkDpkUuAQwXgk9Mjj61LY+gxhnJHQDjaf6fhQOQvIIIHUf0pwBJwBlEOeMc+lLnZyAAf9qlcBhUBlzgYH4mmLtdjhgST685oZsuF5YYGCTSAncwxlcAjPWgk2tFvdjiBmxnJUluB/s1ufaI2U5+8O5NcUzAoeTgHv/jUctxcbOZ5cKc/eNc86PMzojW5VY6q51KOxmWZZir85wevtXPXPiW8bU3laNZIj8qryRtH/wCus0jcxLMWLdWJzUZkRdyshLYwpDYx9a1p0orTczlUbehtv4jV4ykdqqndynHHHriql7rLSxGOJCN3du1ZaSKEPzA88GlcgjdnnpV+zinsRKrLuQs5Mbg/MWHLHk8V9p2P/IPtv+uS/wAhXxVJwpA6V9qWP/IPtv8Arkv8hXXROOr5lmiiitzIyPE4z4Y1Mf8ATtJ/6Ca8GhtbaRpBO7IuOSGxXvXibH/CMann/n2k/wDQTXhEa5dmK/iDXn43dHZhXZMzILeWOWfcS0CsBE7feI9Tip4y7y7UPzFuSxyAKsAq0hjIO7bndt4p6RrliVwx/ixiuZyudLGxgNgEKG9Bzj8akYDkDtnA96UN824AbscnPWkw24NnCkcgn2pXJYsUJkcsclAc9ec1YS+UvsIC47CrcMX+iDaRtOOc1myRrApZFY4HzEHNYSlzM6acUkWi5diVy/RsZxjFRaejvfSjfLyudp+6OefpRDIvlo+QOMgAdQfXFIjPE5kywJ9Mf0rM2NJgzYGMkcZzVm3iX7M6ycqCOAKs6TDHNZQzE/vGHzClujHAPVT6UmQ5a2GJsUfIMseKzGG6QF3/AIskAZ4q15wY+Witk9GGPlpPKjGBkb165PNCC9im1w9qWMLouRxuHQ5qKXUZJl89yMhTknoR61DqUkMDxvKcLu5wOB9aLXSLi9YGVl+xPyWLYLe1XbTUrRI5LxbNNfW+YUDxBeucEHjnH0rD0N40R1IAcHkEn/P4V6Rf21rCXh8mMKnQHsP6155qFjLpN2l0qjaSN+3pz0Nd2HmnH2aOSrGz5zQY44xsyQAMn8BTd7M/y/KcA4FPR8hSDwVyDnqPyppJB4bjp16VVzMkt1V4nd428vOJGUDI98VUZoxkAFSD1P8AKpzkFscAjqO46dahxgcAKMZP9aEDIjn5iS3vmncjnGD1GOv/AOulVSmCpwcDnnNMZSZAD36ZHGfr2o3JuBOT9O5NMmj5U8kHnrSht27g5UdhTpAGC7gAcAZU5zRewEDIChHaopYFC+W+4MOeeDWg/wBmW1xH/rMjgjkcc8+lQ3Fw88JR9jHI+cj5vbmnGWoWsZsMYhyOcHkc5xQzndtA4qQgRERsoGDngDP51DKy+Y2OmflJ649603dzNobKpIxtPtX2pYf8g+2/65L/ACFfE7vg5U4x0r7YsTnT7Y/9Ml/kK6KWhhVLNFFFbGRk+Jf+Ra1LOMfZpM/98mvCgyj5S4+Xrnmva/G8hh8Da5KOqWMxH/fBr5MHiGXHMnviuLE0pTaaOrDySTuekB12D5h1pQysOSDz+VebnxJMuCJCGpw8SzD/AJaNg+1c/wBXkbOpE9Iwp7jHqBS4VjgH7o59q84/4Sabd/rPrxT/APhJ35zISCOcij2E0HtFc9Ohlb7MF9OAMdap3qEKmWOe+ewzXG6f42S1DiaMyZPUHp+FdBFrFpeWiTrKJd54AYBsemPWuaVCcXsdcKkWtyxpt1HMfKJ2ujEYzzjscVfuCpdF3qiEDDetYduf9JMwJjkHJ7cVcF7a3P7qO4XzNu8Rqw3Af0qXB3ui+Y04r37KRbLN8yjJJ4yKVL1GcfaJ9gAO7LdPxrkNYv49JjW5UkHfgjIJbPpmuebxRJPcSSsQu84+7yF9K2hh5TjcxnVjF67npUmpW8M7GK4Hy4JG4ZI9afDqsU0iq5x3zz+VeZ/8JIQScjJOScU0eJXjPyEqexHFU8JLoifrET0W5lW6uHO3cg4C4I598/55rT0mWNDIDKPTAY8GvM4vHFyibcISBgMV5q1oPi3/AImQS7kGyQnBwB8x7k1EsLUsWq8Hod3qIWabI3SbnyS3OT7VzWrWaXJEEqnD8cA8D/62K6dTHLEsgfJ7EGsPW7GSW2MiztG0YLq47H1rKldTSehpOzjoclp0EoFzE0zMiOUQ56dsj0q8FIUDPQdfX8a56z1SWJCrtvAY4JHOasDVdgwoHX8q9GcZXOFTRrK24YHy88+/NHljPQAAVkjVASflGamj1NDHJvAypB+9jPPIFR7OQcyLzfLxyeKYwIbn8sVnHU+WDAA0j3zFNwj+Q9eafs2F0Xw4V+F+UjqDjFPEccke0OQ/pjA9uaoQagzHagjUkH75AB/H86rSagzu5wE9FHQUezkLmReJAyevsKeY0UKWc4Zc5Vc/hWWt/IBjdyO9TDU0eJVkErMvCndwPwp+zlcSaJbyK3mlxHvUhhtDfTn9ary286MGML7RwT6VBJd73ZhGBnqO1XrbUfNjWIl95UhmY8e1W1JIceVspSlY2PlqGVhgq3UV9rWH/IPtv+uS/wAhXxvc2/mx7l/1q9T7V9kWP/Hhb/8AXNf5CtqLujCurMs0UUVuYGV4k0yTWfDWpaZEypJd20kKs3QFlIyfzrwb/hnjXen9p6fj/ef/AOIoopNXGpNbCf8ADO+u/wDQU0/83/8AiaD+zvruONU0/wDN/wD4miilyofOxP8AhnbXiP8AkK6fn6v/APE0n/DOuv8A/QX0783/APiaKKFETmwH7O3iD/oK6b+b/wDxNOT9nnxGjhk1jT1IOQQzj/2WiinyoPaMtP8AArxY8Jh/tyy2HOf3kn/xNVI/2e/EsMyyx6xYK4PDB5Af/QaKKSgloN1JPUfc/ADxVeMDPrWnvjpueQ4/8dqv/wAM6eI/+grpn/fUn/xNFFPlSBzd7gf2dfEnbVdL/wC+pP8A4mk/4Z08S/8AQV0v/vqT/wCJooosLnYn/DOniX/oK6X/AN9Sf/E0o/Z18S/9BXS/++pP/iaKKLBzs1bD4KeNNOiEUOu6f5QO7YWcgH2+WrV18I/HFzA0J1rTArDB5f8A+JooqHSg3zNFqvNKyZgf8M7eJ+o1TSv++5P/AIij/hnfxR21TSv++5P/AIiiirsTzsP+Gd/FGc/2ppX/AH3J/wDEUv8Awzx4o/6Cmlf99yf/ABFFFFkHOw/4Z48Uj/mKaV/33J/8RSt+z14rYAHVtLIAwB5kn/xFFFFkHMxB+zx4pH/MU0r/AL7k/wDiKT/hnfxR/wBBTSv++5P/AIiiiiyDmYf8M7+KP+gppX/fcn/xFKP2ePFA/wCYnpP/AH3J/wDEUUUWQczD/hnjxQf+YnpP/fcn/wART0/Z88UxsrLqmlAr0O6T/wCIooo5UHPIf/woHxaZN51fTCf+ukn/AMRX0TbRGG2ijY5KIFOPYUUUJJbCcnLcmooopgAAAAAAAAI//9k="
    }
   },
   "cell_type": "markdown",
   "id": "329fd4ee-4a68-4f3b-b157-a676f13ba587",
   "metadata": {},
   "source": [
    "![figure-8-1.jpg](attachment:227da97f-e1ae-4252-b577-03a873a321e9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fde6f17-d244-4270-b759-68e1858d399f",
   "metadata": {},
   "source": [
    "We can retrieve this image summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f52ee1e-ed46-4a81-834a-3608a1cf90ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The image shows a humorous meme with a text overlay that reads, \"sometimes I just look at pictures of the earth from space and marvel at how beautiful it all is.\" Below this text is a photo of a tray containing pieces of chicken or fish sticks, which are arranged to form the shape of a map. The arrangement resembles continents on Earth. The meme uses a visual pun by juxtaposing a thoughtful statement with an image of food that has been creatively shaped to mimic a world map from space. The text suggests a sense of wonder and appreciation for the natural world, while the image plays on this sentiment using food as a medium for art. '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"Images / figures with playful and creative examples\")[\n",
    "    0\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69060724-e390-4dda-8250-5f86025c874a",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "Run [RAG pipeline](https://python.langchain.com/docs/expression_language/cookbook/retrieval).\n",
    "\n",
    "For `option 1` (above): \n",
    "\n",
    "* Simply pass retrieved text chunks to LLM, as usual.\n",
    "\n",
    "For `option 2a` (above): \n",
    "\n",
    "* We would pass retrieved image and images to the multi-modal LLM.\n",
    "* This should be possible soon, once [llama-cpp-python add multi-modal support](https://github.com/abetlen/llama-cpp-python/issues/813)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b09cb2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "771a47fa-1267-4db8-a6ae-5fde48bbc069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Option 1: LLM\n",
    "model = ChatOllama(model=\"mistral\",temperature=0)\n",
    "# Option 2: Multi-modal LLM\n",
    "# model = LLaVA\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea8414a8-65ee-4e11-8154-029b454f46af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" LLaVA was evaluated on a diverse set of 24 images with 60 questions in total, covering indoor and outdoor scenes, memes, paintings, sketches, etc. The results showed that LLaVA significantly outperforms BLIP-2 and OpenFlamingo in Table 5. Compared to the text-only GPT-4 model, LLaVA achieves an impressive 81.7% performance on complex reasoning questions with an overall score of 67.3%. However, it is important to note that this benchmark was designed to reveal a model's weaknesses and serve as a solid baseline for future work in developing more capable LMMs. There were also challenging examples, such as the ramen example which required multilingual understanding capability and extensive knowledge coverage, and the fridge example which required high resolution image processing and extensive knowledge coverage (Table 6). Overall, these results demonstrate the effectiveness of LLaVA for handling various image domains and its potential to inspire future work in developing more advanced LMMs.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    \"What is the performance of LLaVa across across multiple image domains / subjects?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7aeb57-2ab8-496c-b909-0734ccc5da5f",
   "metadata": {},
   "source": [
    "We can check the [trace](https://smith.langchain.com/public/ab90fb1c-5949-4fc6-a002-56a6056adc6b/r) to review retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ad375c5-8aef-4be3-9a12-8ad953fa2d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Sure, I\\'d be happy to help! Based on the provided context, here are some playful and creative explanations for the images/figures mentioned in the paper:\\n\\n1. \"The image features a close-up of a tray filled with various pieces of fried chicken. The chicken pieces are arranged in a way that resembles a map of the world, with some pieces placed in the shape of continents and others as countries.\"\\n\\nPlayful explanation: \"Look, ma! The fried chicken is mapping out the world one piece at a time! Who needs Google Maps when you have crispy chicken wings to guide the way?\"\\n\\nCreative explanation: \"The arrangement of the fried chicken pieces creates a visual representation of the world that\\'s both appetizing and adventurous. It\\'s like a culinary globe-trotting experience!\"\\n\\n2. \"The image is a screenshot of a conversation between two people, likely discussing a painting.\"\\n\\nPlayful explanation: \"The painting is getting a double take - these two people are having a chat about it and we get to eavesdrop on their art-loving banter!\"\\n\\nCreative explanation: \"This image captures the dynamic exchange of ideas between two art enthusiasts. It\\'s like we\\'re peeking into their creative brainstorming session, where the painting is the catalyst for a lively discussion.\"\\n\\n3. \"The image features a text-based representation of a scene with a person holding onto a rope, possibly a woman, and a boat in the background.\"\\n\\nPlayful explanation: \"This image looks like a page from a choose-your-own-adventure book! Is our brave protagonist about to embark on a thrilling boat ride or hold tight for a wild journey?\"\\n\\nCreative explanation: \"The text-based representation of the scene creates an intriguing narrative that invites the viewer to fill in the blanks. It\\'s like we\\'re reading a visual storybook, where the person holding onto the rope is the hero of their own adventure.\"\\n\\n4. \"Figure 5: LLaVA recognizes the famous art work, Mona Lisa, by Leonardo da Vinci.\"\\n\\nPlayful explanation: \"Mona Lisa is getting a digital spotlight - look at her smile now that she\\'s part of this cool image recognition tech!\"\\n\\nCreative explanation: \"This playful recognition of the Mona Lisa painting highlights the advanced technology used in image analysis. It\\'s like LLaVA is giving the famous artwork a modern makeover, showcasing its timeless beauty and relevance in the digital age.\"\\n\\nOverall, these images/figures offer unique opportunities for creative and playful explanations that can capture the viewer\\'s attention while highlighting the technology and narratives presented in the paper.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    \"Explain any images / figures in the paper with playful and creative examples.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a4abe8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Image 1: The first image is a meme that uses a visual pun to combine two seemingly unrelated things: looking at beautiful pictures of the Earth from space and eating chicken nuggets. In this meme, the text says \"Sometimes I just look at pictures of the earth from space and marvel at how beautiful it all is.\" Below this text is an image of a tray with chicken or fish sticks arranged to resemble a world map. This playful and creative example uses food as a medium for art, creating a humorous representation of the Earth. The overall impression given by this meme is one of wonder and appreciation for both the natural world and the simple pleasure of eating tasty food.\\n\\nImage 2: In the second image, we see a screenshot of a messaging application with two individuals engaged in a conversation. One user, named \"Law,\" sends a message containing a photo of Leonardo da Vinci\\'s famous painting \"The Mona Lisa.\" The other user uses a voice assistant or AI, as indicated by the text bubble that says \"You know what, I\\'m feeling pretty inspired now and I want you to show me some really cool art pieces.\" In response, the AI provides a picture of a dog that has been artistically painted in various styles, including impressionism, cubism, and surrealism. The humorous text bubble above this image reads \"I\\'m not sure what you mean by \\'really cool art pieces.\\' Do you want me to find some that have nothing to do with cats?\" This example showcases the creative use of a voice assistant or AI to generate art-related content and the unexpected twist of using dogs instead of the usual cat-themed results.\\n\\nThese examples demonstrate how images can be used playfully and creatively in various contexts, such as memes, messaging applications, and even educational materials. By combining unrelated things in new and unexpected ways, these visual elements add humor, intrigue, and delight to our daily interactions.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    \"Explain any images / figures in the paper with playful and creative examples.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd71d757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' According to the context provided, the GPT-3.5 model has a human-like score of 90.23%, and it was evaluated on ScienceQA dataset with multimodal multiple choice questions. However, the text does not provide specific values for subject, context modality, or grade for GPT-3.5 in the table format you are looking for. The average performance of GPT-3.5 is 74.64% and 75.44% with and without chain-of-thought (CoT), respectively.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    \"provide me the metric values of gpt 3.5 like subject , context modality and grade and average\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9540e81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' To answer your question, I\\'d need to know what \"before\" and \"last\" refer to specifically in this context. Based on the provided information, it seems that \"before\" could refer to a previous version or state, while \"last\" could refer to the most recent version or state. However, without more context or specific visuals, it\\'s not possible to provide you with the exact visual features of \"before\" and \"last.\"\\n\\nAdditionally, it\\'s important to note that visual features can encompass a wide range of characteristics, including but not limited to: color, texture, shape, size, position, orientation, symmetry, asymmetry, and more. Depending on the specific context or domain, some visual features may be more relevant or informative than others.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    \"provide me the visual features of before and last\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da79644-4046-45b0-8c25-01aa73587b22",
   "metadata": {},
   "source": [
    "We can check the [trace](https://smith.langchain.com/public/c6d3b7d5-0f40-4905-ab8f-3a2b77c39af4/r) to review retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31e17d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Chicken Nugget Map meme is a humorous image that uses chicken nuggets arranged on a tray to resemble a world map. The meme begins with text expressing admiration for the beauty of Earth from space. However, instead of an actual Earth image, the main part of the meme shows a tray of cooked chicken nuggets where each piece is positioned to represent continents and islands.\n",
      "\n",
      "The humor lies in the unexpected juxtaposition of the text praising the earth's beauty with the mundane and unrelated image of chicken nuggets. The punchline, which appears at the bottom in smaller text, reads \"I mean, it’s not the real Earth, but how beautiful it is all is,\" adding to the comedic effect. Overall, this meme combines food and humor, creating a fun and imaginative representation of the world."
     ]
    }
   ],
   "source": [
    "async for chunk in chain.astream(\"tell me about the Chicken Nugget Map\"):\n",
    "#     chunks.append(chunk)\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80071b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The CC3M (Conceptual Captions 3M) dataset is a large-scale multimodal benchmark for evaluating the ability of models to generate captions that accurately describe images. It consists of approximately 3 million image-caption pairs, each with an associated concept label drawn from a vocabulary of around 50,000 concepts [62]. The dataset is split into training, validation, and test sets, with roughly 2.4M, 178K, and 500K image-caption pairs, respectively. The CC3M dataset is widely used for evaluating the performance of various multimodal models in understanding images and generating descriptive captions."
     ]
    }
   ],
   "source": [
    "async for chunk in chain.astream(\"tell me about  CC3M dataset\"):\n",
    "#     chunks.append(chunk)\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f518f15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The given text appears to be a summary of a research paper or article related to language models and computer vision. It discusses various models, benchmarks, and limitations in the field. Here is a brief explanation of the content:\n",
      "\n",
      "The text starts by mentioning some recent advancements in multimodal language models (LLMs) like LLaVA, PaLM-E, and Make-a-Scene. It then introduces two benchmarks, LLaVA-Bench (COCO) and LLaVA-Bench (In-the-Wild), to evaluate the performance of these models in handling visual instructions.\n",
      "\n",
      "LLaVA-Bench (COCO) involves randomly selecting 30 images from COCO-Val-2014 and generating three types of questions for each image, totaling 90 questions. The benchmark aims to study the model's alignment behavior and capabilities with consistent visual inputs. The text discusses improvements in the model's performance when using instruction tuning, adding detailed description and complex reasoning questions, and having all three types of data.\n",
      "\n",
      "LLaVA-Bench (In-the-Wild) collects a diverse set of 24 images with 60 questions in total to evaluate the model's capability in more challenging tasks and generalizability to novel domains. The text compares LLaVA, BLIP, and OpenFlamingo on this benchmark and shows that LLaVA achieves significantly better performance compared with BLIP-2 (+29%) and OpenFlamingo (+48%).\n",
      "\n",
      "The text then discusses some limitations of the LLaVA-Bench (In-the-Wild), providing examples to illustrate the challenges in understanding complex semantics within images, multilingual understanding, and knowledge coverage. The text concludes by expressing hope that LLaVA serves as a solid baseline for future work on developing more capable LLMs.\n",
      "\n",
      "The text also includes references to various research papers and resources for further reading."
     ]
    }
   ],
   "source": [
    "async for chunk in chain.astream(\"tell me about  Figure 7\"):\n",
    "#     chunks.append(chunk)\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7a809fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To answer your question, I'd need to access the CC3M dataset and perform some data analysis. However, based on the context of the paper you provided, LLaVA is a large language model that can generate instructions and follow them in the context of visual information. It was evaluated on two benchmarks: LLaVA-Bench (COCO) and LLaVA-Bench (In-the-Wild). The first benchmark involved generating questions for 30 images from COCO-Val-2014, while the second benchmark involved a diverse set of 24 images with manually-curated descriptions and associated questions.\n",
      "\n",
      "Regarding your question about frequency and unique noun-phrases in CC3M, I would suggest looking up the paper or dataset documentation for more information. The CC3M dataset is a large-scale multimodal dataset that contains 14 million image-text pairs scraped from ConceptNet and CommonCrawl. It was used as one of the training datasets in the LLaVA study, but it wasn't specifically mentioned in relation to your question about frequency and unique noun-phrases.\n",
      "\n",
      "If you have access to the CC3M dataset, you can use text analysis tools or libraries such as NLTK (Natural Language Toolkit) or spaCy to extract and analyze the text data. For example, you can use NL"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chain\u001b[38;5;241m.\u001b[39mastream(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtell me about frequency and Unique noun-phrases (ordered by frequency in the descending order) for CC3M\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#     chunks.append(chunk)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(chunk, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2900\u001b[0m, in \u001b[0;36mRunnableSequence.astream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2897\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput_aiter\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Input]:\n\u001b[0;32m   2898\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m-> 2900\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matransform(input_aiter(), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2901\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2883\u001b[0m, in \u001b[0;36mRunnableSequence.atransform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2877\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21matransform\u001b[39m(\n\u001b[0;32m   2878\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2879\u001b[0m     \u001b[38;5;28minput\u001b[39m: AsyncIterator[Input],\n\u001b[0;32m   2880\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2881\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   2882\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Output]:\n\u001b[1;32m-> 2883\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atransform_stream_with_config(\n\u001b[0;32m   2884\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2885\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atransform,\n\u001b[0;32m   2886\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   2887\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2888\u001b[0m     ):\n\u001b[0;32m   2889\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1979\u001b[0m, in \u001b[0;36mRunnable._atransform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1977\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1978\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_context(asyncio\u001b[38;5;241m.\u001b[39mcreate_task):\n\u001b[1;32m-> 1979\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1980\u001b[0m             py_anext(iterator),  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1981\u001b[0m             context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m   1982\u001b[0m         )\n\u001b[0;32m   1983\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1984\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m cast(Output, \u001b[38;5;28;01mawait\u001b[39;00m py_anext(iterator))\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2853\u001b[0m, in \u001b[0;36mRunnableSequence._atransform\u001b[1;34m(self, input, run_manager, config)\u001b[0m\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[0;32m   2846\u001b[0m     final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39matransform(\n\u001b[0;32m   2847\u001b[0m         final_pipeline,\n\u001b[0;32m   2848\u001b[0m         patch_config(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2851\u001b[0m         ),\n\u001b[0;32m   2852\u001b[0m     )\n\u001b[1;32m-> 2853\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_pipeline:\n\u001b[0;32m   2854\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\output_parsers\\transform.py:60\u001b[0m, in \u001b[0;36mBaseTransformOutputParser.atransform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21matransform\u001b[39m(\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28minput\u001b[39m: AsyncIterator[Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage]],\n\u001b[0;32m     57\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m     59\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[T]:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atransform_stream_with_config(\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atransform, config, run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m     ):\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1979\u001b[0m, in \u001b[0;36mRunnable._atransform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1977\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1978\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_context(asyncio\u001b[38;5;241m.\u001b[39mcreate_task):\n\u001b[1;32m-> 1979\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1980\u001b[0m             py_anext(iterator),  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1981\u001b[0m             context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m   1982\u001b[0m         )\n\u001b[0;32m   1983\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1984\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m cast(Output, \u001b[38;5;28;01mawait\u001b[39;00m py_anext(iterator))\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\output_parsers\\transform.py:38\u001b[0m, in \u001b[0;36mBaseTransformOutputParser._atransform\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_atransform\u001b[39m(\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: AsyncIterator[Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage]]\n\u001b[0;32m     37\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[T]:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunk, BaseMessage):\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([ChatGeneration(message\u001b[38;5;241m=\u001b[39mchunk)])\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\utils\\aiter.py:97\u001b[0m, in \u001b[0;36mtee_peer\u001b[1;34m(iterator, buffer, peers, lock)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m iterator\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1333\u001b[0m, in \u001b[0;36mRunnable.atransform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1326\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1327\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed while trying to add together \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1328\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(final)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(chunk)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1329\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThese types should be addable for atransform to work.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1330\u001b[0m             )\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1333\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mastream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1334\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:313\u001b[0m, in \u001b[0;36mBaseChatModel.astream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(\n\u001b[0;32m    310\u001b[0m         e,\n\u001b[0;32m    311\u001b[0m         response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]] \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;28;01melse\u001b[39;00m []),\n\u001b[0;32m    312\u001b[0m     )\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[0;32m    316\u001b[0m         LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]]),\n\u001b[0;32m    317\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:291\u001b[0m, in \u001b[0;36mBaseChatModel.astream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m generation: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_astream(\n\u001b[0;32m    292\u001b[0m         messages,\n\u001b[0;32m    293\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    295\u001b[0m     ):\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m             chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_manager\u001b[38;5;241m.\u001b[39mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:335\u001b[0m, in \u001b[0;36mChatOllama._astream\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_astream\u001b[39m(\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    330\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[ChatGenerationChunk]:\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acreate_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    336\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[0;32m    337\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _chat_stream_response_to_chat_generation_chunk(stream_resp)\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:174\u001b[0m, in \u001b[0;36mChatOllama._acreate_chat_stream\u001b[1;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acreate_chat_stream\u001b[39m(\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    167\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m    168\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    170\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    171\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_messages_to_ollama_messages(messages),\n\u001b[0;32m    173\u001b[0m     }\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acreate_stream(\n\u001b[0;32m    175\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload, stop\u001b[38;5;241m=\u001b[39mstop, api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    176\u001b[0m     ):\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m stream_resp\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:318\u001b[0m, in \u001b[0;36m_OllamaCommon._acreate_stream\u001b[1;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m         optional_detail \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    314\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    315\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama call failed with status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    316\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Details: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptional_detail\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    317\u001b[0m         )\n\u001b[1;32m--> 318\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m line\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\aiohttp\\streams.py:44\u001b[0m, in \u001b[0;36mAsyncStreamIterator.__anext__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__anext__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m         rv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_func()\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EofStream:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\aiohttp\\streams.py:307\u001b[0m, in \u001b[0;36mStreamReader.readline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadline\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreaduntil()\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\aiohttp\\streams.py:341\u001b[0m, in \u001b[0;36mStreamReader.readuntil\u001b[1;34m(self, separator)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_enough:\n\u001b[1;32m--> 341\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreaduntil\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunk\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\aiohttp\\streams.py:302\u001b[0m, in \u001b[0;36mStreamReader._wait\u001b[1;34m(self, func_name)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timer:\n\u001b[1;32m--> 302\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "async for chunk in chain.astream(\"tell me about frequency and Unique noun-phrases (ordered by frequency in the descending order) for CC3M\"):\n",
    "#     chunks.append(chunk)\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94feef2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are the unique noun phrases (in order of their frequency from highest to lowest) found in the given text:\n",
      "\n",
      "1. language model\n",
      "2. few-shot learning\n",
      "3. visual concepts\n",
      "4. instruction-finetuned language models\n",
      "5. open-source chatbot\n",
      "6. pathways\n",
      "7. web-scale image-text pre-training\n",
      "8. long-tail visual concepts\n",
      "9. multimodal language model\n",
      "10. embodied multimodal language model\n",
      "11. scene-based text-to-image generation\n",
      "12. vision-language pre-training\n",
      "13. computer vision in the wild\n",
      "14. reinforce data\n",
      "15. model accuracy and robustness\n",
      "16. dataset reinforcement\n",
      "17. human priors\n",
      "18. make-a-scene\n",
      "19. scene-based text-to-image generation\n",
      "20. text-annotation tasks\n",
      "21. crowd-workers\n",
      "22. text-to-image generation\n",
      "23. model accuracy\n",
      "24. robustness\n",
      "25. dataset reinforcement\n",
      "26. human priors\n",
      "27. scene-based\n",
      "28. text-annotation\n",
      "29. text-to-image\n",
      "30. image-text pre-training\n",
      "31. recent advances\n",
      "32. future trends\n",
      "33. Foundations and Trends® in Computer Graphics and Vision\n",
      "34. Computer Vision in the Wild\n",
      "35. CVinW_Readings\n",
      "36. Palm: Scaling language modeling with pathways\n",
      "37. instruction-finetuned\n",
      "38. Scaling instruction-finetuned language models\n",
      "39. Aakanksha Chowdhery\n",
      "40. Sharan Narang\n",
      "41. Jacob Devlin\n",
      "42. Maarten Bosma\n",
      "43. Gaurav Mishra\n",
      "44. Adam Roberts\n",
      "45. Paul Barham\n",
      "46. Hyung Won Ch"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chain\u001b[38;5;241m.\u001b[39mastream(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnique noun-phrases (ordered by frequency in the descending order)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#     chunks.append(chunk)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(chunk, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2900\u001b[0m, in \u001b[0;36mRunnableSequence.astream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2897\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput_aiter\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Input]:\n\u001b[0;32m   2898\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m-> 2900\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matransform(input_aiter(), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2901\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2883\u001b[0m, in \u001b[0;36mRunnableSequence.atransform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2877\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21matransform\u001b[39m(\n\u001b[0;32m   2878\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2879\u001b[0m     \u001b[38;5;28minput\u001b[39m: AsyncIterator[Input],\n\u001b[0;32m   2880\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2881\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   2882\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Output]:\n\u001b[1;32m-> 2883\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atransform_stream_with_config(\n\u001b[0;32m   2884\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2885\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atransform,\n\u001b[0;32m   2886\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   2887\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2888\u001b[0m     ):\n\u001b[0;32m   2889\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1979\u001b[0m, in \u001b[0;36mRunnable._atransform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1977\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1978\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_context(asyncio\u001b[38;5;241m.\u001b[39mcreate_task):\n\u001b[1;32m-> 1979\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1980\u001b[0m             py_anext(iterator),  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1981\u001b[0m             context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m   1982\u001b[0m         )\n\u001b[0;32m   1983\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1984\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m cast(Output, \u001b[38;5;28;01mawait\u001b[39;00m py_anext(iterator))\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2853\u001b[0m, in \u001b[0;36mRunnableSequence._atransform\u001b[1;34m(self, input, run_manager, config)\u001b[0m\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[0;32m   2846\u001b[0m     final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39matransform(\n\u001b[0;32m   2847\u001b[0m         final_pipeline,\n\u001b[0;32m   2848\u001b[0m         patch_config(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2851\u001b[0m         ),\n\u001b[0;32m   2852\u001b[0m     )\n\u001b[1;32m-> 2853\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_pipeline:\n\u001b[0;32m   2854\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\output_parsers\\transform.py:60\u001b[0m, in \u001b[0;36mBaseTransformOutputParser.atransform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21matransform\u001b[39m(\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28minput\u001b[39m: AsyncIterator[Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage]],\n\u001b[0;32m     57\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m     59\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[T]:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atransform_stream_with_config(\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atransform, config, run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m     ):\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1979\u001b[0m, in \u001b[0;36mRunnable._atransform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1977\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1978\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_context(asyncio\u001b[38;5;241m.\u001b[39mcreate_task):\n\u001b[1;32m-> 1979\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1980\u001b[0m             py_anext(iterator),  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1981\u001b[0m             context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m   1982\u001b[0m         )\n\u001b[0;32m   1983\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1984\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m cast(Output, \u001b[38;5;28;01mawait\u001b[39;00m py_anext(iterator))\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\output_parsers\\transform.py:38\u001b[0m, in \u001b[0;36mBaseTransformOutputParser._atransform\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_atransform\u001b[39m(\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: AsyncIterator[Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage]]\n\u001b[0;32m     37\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[T]:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunk, BaseMessage):\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([ChatGeneration(message\u001b[38;5;241m=\u001b[39mchunk)])\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\utils\\aiter.py:97\u001b[0m, in \u001b[0;36mtee_peer\u001b[1;34m(iterator, buffer, peers, lock)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m iterator\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1333\u001b[0m, in \u001b[0;36mRunnable.atransform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1326\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1327\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed while trying to add together \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1328\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(final)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(chunk)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1329\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThese types should be addable for atransform to work.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1330\u001b[0m             )\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1333\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mastream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1334\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:313\u001b[0m, in \u001b[0;36mBaseChatModel.astream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(\n\u001b[0;32m    310\u001b[0m         e,\n\u001b[0;32m    311\u001b[0m         response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]] \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;28;01melse\u001b[39;00m []),\n\u001b[0;32m    312\u001b[0m     )\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[0;32m    316\u001b[0m         LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]]),\n\u001b[0;32m    317\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:291\u001b[0m, in \u001b[0;36mBaseChatModel.astream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m generation: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_astream(\n\u001b[0;32m    292\u001b[0m         messages,\n\u001b[0;32m    293\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    295\u001b[0m     ):\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m             chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_manager\u001b[38;5;241m.\u001b[39mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:335\u001b[0m, in \u001b[0;36mChatOllama._astream\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_astream\u001b[39m(\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    330\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[ChatGenerationChunk]:\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acreate_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    336\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[0;32m    337\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _chat_stream_response_to_chat_generation_chunk(stream_resp)\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:174\u001b[0m, in \u001b[0;36mChatOllama._acreate_chat_stream\u001b[1;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acreate_chat_stream\u001b[39m(\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    167\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m    168\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    170\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    171\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_messages_to_ollama_messages(messages),\n\u001b[0;32m    173\u001b[0m     }\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acreate_stream(\n\u001b[0;32m    175\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload, stop\u001b[38;5;241m=\u001b[39mstop, api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    176\u001b[0m     ):\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m stream_resp\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:318\u001b[0m, in \u001b[0;36m_OllamaCommon._acreate_stream\u001b[1;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m         optional_detail \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    314\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    315\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama call failed with status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    316\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Details: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptional_detail\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    317\u001b[0m         )\n\u001b[1;32m--> 318\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m line\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\aiohttp\\streams.py:44\u001b[0m, in \u001b[0;36mAsyncStreamIterator.__anext__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__anext__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m         rv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_func()\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EofStream:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\aiohttp\\streams.py:307\u001b[0m, in \u001b[0;36mStreamReader.readline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadline\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreaduntil()\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\aiohttp\\streams.py:341\u001b[0m, in \u001b[0;36mStreamReader.readuntil\u001b[1;34m(self, separator)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_enough:\n\u001b[1;32m--> 341\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreaduntil\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunk\n",
      "File \u001b[1;32mc:\\Users\\Barani\\Desktop\\chatbot\\multimodel\\Lib\\site-packages\\aiohttp\\streams.py:302\u001b[0m, in \u001b[0;36mStreamReader._wait\u001b[1;34m(self, func_name)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timer:\n\u001b[1;32m--> 302\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "async for chunk in chain.astream(\"Unique noun-phrases (ordered by frequency in the descending order)\"):\n",
    "#     chunks.append(chunk)\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a3c47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
